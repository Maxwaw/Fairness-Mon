# -*- coding: utf-8 -*-
"""Dataprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SXbQ0UZLRqTb9iDloDq0zhfkNT20QtXp
"""

import numpy as np
import pandas as pd

from abc import abstractproperty,abstractmethod,ABC

"""
General Class definition for Dataprocessing

"""
class Dataprocessing(ABC):
  @abstractmethod
  #abstractproperty
  def get_data(self):
    #features = Numpy Array of size n x k, where n is the number of samples and k is the (maximum) number of features of the datasamples.
    #labels = Numpy Array of size n, where n is the number of samples.
    #bonus_information = Dictionary that contains additional information that may need to be passed to other methods/steps.
    #return features,labels,bonus_information
    pass
  @abstractmethod
  #abstractproperty
  def preprocess(self):
    pass
  @abstractmethod
  #abstractproperty
  def in_process(self):
    pass
  @abstractmethod
  #abstractproperty
  def postprocess(self):
    pass

  def overall_processing(self):
    numpy_data_array,ground_truth_labels,bonus_information = self.get_data

    numpy_data_array,ground_truth_labels,bonus_information = self.preprocess(numpy_data_array,ground_truth_labels,bonus_information)

    numpy_data_array,ground_truth_labels,bonus_information = self.in_process(numpy_data_array,ground_truth_labels,bonus_information)

    numpy_data_array,ground_truth_labels,bonus_information = self.post_process(numpy_data_array,ground_truth_labels,bonus_information)
    return numpy_data_array,ground_truth_labels,bonus_information

"""
Specific Example for Dataprocessing

Here the US Census (New 'Adult' Dataset)
"""
class US_Census_Dataprocessing(Dataprocessing):
  def __init__(self):
    self.MyHelper = Data_Helper_US_Census()

  def get_data(self,states,years,input_threshold=50000,group_non_white=False,return_original_income=False):
    #numpy_data_array  = np.zeros((0,10))
    #ground_truth_labels = np.zeros((0))
    #bonus_information = {}

    self.numpy_data_array,self.ground_truth_labels,self.bonus_information = self.MyHelper.get_income_data_with_track(states,years,input_threshold=input_threshold,group_non_white=group_non_white,horizon='1-Year',survey='person',return_original_income=return_original_income):

    
    return self.numpy_data_array,self.ground_truth_labels,self.bonus_information 

  def get_supported_states(self):
    return self.MyHelper.supported_states


  def preprocess(self, numpy_data_array=None,ground_truth_labels=None,bonus_information=None):
    if numpy_data_array==None:
      numpy_data_array = self.numpy_data_array
    if ground_truth_labels==None:
      ground_truth_labels = self.ground_truth_labels
    if bonus_information==None:
      bonus_information = self.bonus_information 
  
    return numpy_data_array,ground_truth_labels,bonus_information
  def in_process(self, numpy_data_array=None,ground_truth_labels=None,bonus_information=None):
    if numpy_data_array==None:
      numpy_data_array = self.numpy_data_array
    if ground_truth_labels==None:
      ground_truth_labels = self.ground_truth_labels
    if bonus_information==None:
      bonus_information = self.bonus_information 
  
    return numpy_data_array,ground_truth_labels,bonus_information

  def postprocess(self, numpy_data_array=None,ground_truth_labels=None,bonus_information=None):
    if numpy_data_array==None:
      numpy_data_array = self.numpy_data_array
    if ground_truth_labels==None:
      ground_truth_labels = self.ground_truth_labels
    if bonus_information==None:
      bonus_information = self.bonus_information 
  
    return numpy_data_array,ground_truth_labels,bonus_information

   def overall_processing(self):
    numpy_data_array,ground_truth_labels,bonus_information = self.get_data

    numpy_data_array,ground_truth_labels,bonus_information = self.preprocess(numpy_data_array,ground_truth_labels,bonus_information)

    numpy_data_array,ground_truth_labels,bonus_information = self.in_process(numpy_data_array,ground_truth_labels,bonus_information)

    numpy_data_array,ground_truth_labels,bonus_information = self.post_process(numpy_data_array,ground_truth_labels,bonus_information)
    return numpy_data_array,ground_truth_labels,bonus_information

"""
A Data Helper that is used to 'outsource' various smaller methods.
"""

from Levenshtein import distance as levenshtein_distance
from aif360.datasets import BinaryLabelDataset
from folktables import ACSDataSource, ACSIncome, BasicProblem


class Data_Helper_US_Census():

  def __init__(self,label_name = ["Label"],feature_names = ["AGEP","COW","SCHL","MAR","OCCP","POBP","RELP","WKHP","SEX","RAC1P"],protected_attribute_names =['RAC1P'],privileged_protected_value = [1],verbose=1,state_name_to_code=None,supported_states = None):
      #self.features = features
      #self.labels = labels
      self.label_name = label_name
      self.feature_names = feature_names
      self.protected_attribute_names = protected_attribute_names
      self.privileged_protected_attributes = privileged_protected_value
      self.verbose = verbose
      if supported_states == None:
        self.supported_states = ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","District of Columbia",
                                 "Florida","Georgia","Hawaii","Idaho","Illinois","Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland",
                                 "Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey",
                                 "New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania","Rhode Island",
                                 "South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin",
                                 "Wyoming","Puerto Rico"]
      else:
        self.supported_states = supported_states

      if state_name_to_code == None:
        self.state_name_to_code = state_name_to_code = {
            "Alabama" : "AL", "Alaska" : "AK","Arizona" : "AZ", "Arkansas" : "AR", "California" : "CA", "Colorado" : "CO", "Connecticut" : "CT",
            "Delaware" : "DE","District of Columbia" : "DC","Florida" : "FL","Georgia" : "GA","Hawaii" : "HI","Idaho" : "ID","Illinois" : "IL",
            "Indiana" : "IN","Iowa" : "IA","Kansas" : "KS","Kentucky" : "KY","Louisiana" : "LA","Maine" : "ME","Maryland" : "MD",
            "Massachusetts" : "MA","Michigan" : "MI","Minnesota" : "MN","Mississippi" : "MS","Missouri" : "MO","Montana" : "MT","Nebraska" : "NE","Nevada" : "NV",
            "New Hampshire" : "NH","New Jersey" : "NJ","New Mexico" : "NM", "New York" : "NY","North Carolina" : "NC", "North Dakota" : "ND","Ohio" : "OH",
            "Oklahoma" : "OK", "Oregon" : "OR", "Pennsylvania" : "PA", "Rhode Island" : "RI", "South Carolina" : "SC", "South Dakota" : "SD", "Tennessee" : "TN", "Texas" : "TX",
            "Utah" : "UT", "Vermont" : "VT", "Virginia" : "VA", "Washington" : "WA", "West Virginia" : "WV", "Wisconsin" : "WI", "Wyoming" : "WY", "Puerto Rico" : "PR",
            }
      else:
        self.state_name_to_code= state_name_to_code

      return
  
  def __print_attribute_error_message(self,err):
    """
    Used to print error messages when a certain attribute has not yet been defined
    """
    attribute_name = err.args[0].split("'")[-2:-1][0]
    print(attribute_name, 'not yet created. You can create it using set_',end='')
    print(attribute_name.replace(' ','_'),end='')
    print('.')
    return

  #def label_numpy_data(self,data_array=None,feature_names=None):
  def set_pandas_df(self,numpy_data_array=None,ground_truth_labels=None,feature_names=None,label_name = None):
    """
    Creates an pandas dataframe of the given data/labels and sets it as a attribute of the helper class
    """
    try:
      if feature_names ==None:
          feature_names = self.feature_names
      if numpy_data_array == None:
          numpy_data_array = self.numpy_data_array
      if ground_truth_labels == None:
          ground_truth_labels = self.ground_truth_labels
      if label_name == None:
        label_name=self.label_name
    except AttributeError as err:
      self.__print_attribute_error_message(err)
      raise err

    df_label = pd.DataFrame(ground_truth_labels,columns=label_name)
    df_data = pd.DataFrame(data = numpy_data_array, columns = feature_names)
    self.pandas_df = pd.concat([df_data,df_label],axis=1)
    return self.pandas_df

  def get_pandas_df(self):
    """
    returns the pandas dataframe attribute of the helper class
    """
    try:
      return self.pandas_df
    except AttributeError as err:
      print('Pandas Dataframe not yet created. You can create it using set_pandas_df.')
      raise err
  #Is the above a best practice??
  
  def set_aif_df(self,numpy_data_array=None,ground_truth_labels=None,feature_names = None,label_name=None,protected_attribute_names =None,privileged_protected_value = None,verbose=None):
    """
    Creates an AIF360 BinaryLabel dataframe of the given data/labels and sets it as a attribute of the helper class
    """
    try:
      if numpy_data_array == None:
          numpy_data_array = self.numpy_data_array
      if ground_truth_labels == None:
          ground_truth_labels = self.ground_truth_labels
      if feature_names == None:
        feature_names = self.feature_names
      if label_name == None:
        label_name = self.label_name
      if protected_attribute_names == None:
        protected_attribute_names = self.protected_attribute_names
      if privileged_protected_value == None:
        privileged_protected_value = self.privileged_protected_value
      if verbose == None:
        verbose = self.verbose
    except AttributeError as err:
      self.__print_attribute_error_message(err)
      raise err
    
    try:
      if verbose>0:
        print('Trying to create Pandas Dataframe.')
      pd_df_all = self.set_pandas_df(numpy_data_array=numpy_data_array,ground_truth_labels=ground_truth_labels,feature_names=feature_names,label_name = label_name)
    except AttributeError as err:
        raise err

    self.aif_df = BinaryLabelDataset(df= pd_df_all,label_names = label_name,protected_attribute_names =protected_attribute_names,privileged_protected_attributes = privileged_protected_value)
    return self.aif_df

  def get_aif_df(self):
    """
    returns the AIF360 BinaryLabel dataframe attribute of the helper class
    """
    try:
      return self.aif_df
    except AttributeError as err:
      print('AIF360 Dataframe not yet created. You can create it using set_aif_df.')
      raise err

  def __calc_nearest_state(self,input_list):
    """function that calculates the nearest state given an input string list (e.g. "washintong" would (hopefully) be mapped to "Washington" """

    """
    Example:
    calc_nearest_state(["washingon","new macico"])
    returns -> ['Washington', 'New Mexico']
    """
    output_list = input_list.copy()

    for i in range(len(input_list)):
      cur_nearest_state = None
      cur_score = np.infty
      for state in self.supported_states:
        new_score = levenshtein_distance(input_list[i],state)
        if new_score<cur_score:
          cur_score = new_score
          cur_nearest_state = state

      output_list[i] = cur_nearest_state
    return output_list

  def __get_state_code(self,states):
    """function that returns the state abbreviatian given the full name of a state."""
    state_code_list = []

    for state in states:
      if len(str(state))==2:
        state_code_list.append(state)
      else:
        cur_state= self.__calc_nearest_state([state])[0]
        state_code_list.append(self.state_name_to_code[cur_state])

    return state_code_list

  def __regions_to_states(regions: list):
    """ Gets regions and returns the equivalent states for the regions

    Args:
        regions (list): gets a list of regions

    Returns:
        list: gives a list of list where every list stand for a region with his states

    Example:
        regions_to_states(['west coast geo', 'urban']) ->  [['Alaska', 'California', 'Hawaii', 'Oregon', 'Washington'],['California ','New Jersey ',...]]
    """
    states = []
    for cur_dataset in regions:
        #Determine current state list according to dataset preset
        if cur_dataset == 'west coast geo':
          states.append(["Alaska", "California", "Hawaii", "Oregon", "Washington"])
        elif cur_dataset == 'west coast wiki':
          states.append(["Alaska", "Arizona", "California", "Colorado", "Hawaii", "Idaho", "Montana", "Nevada", "New Mexiko", "Oregon", "Utah", "Washington", "Wyoming"])
        elif cur_dataset == 'east coast geo':
          states.append(["Maine", "New Hampshire", "Massachusetts", "Rhode Island", "Connecticut", "New York", "New Jersey", "Delaware", "Maryland", "Virginia", "North Carolina", "South Carolina", "Georgia", "Florida"])
        elif cur_dataset == 'none coast geo':
          states.append(["Alabama", "Arkansas", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", "Michigan", "Minnesota", "Mississippi", "Missouri", "Nebraska", "North Dakota", "Ohio", "Oklahoma", "Pennsylvania", "South Dakota", "Tennessee", "Texas", "Vermont", "West Virginia", "Wisconsin", "Puerto Rico", "Colorado", "Wyoming", "Montana", "Idaho", "Utah", "Nevada", "Arizona", "New Mexiko"])
        elif cur_dataset == 'none coast wiki':
          states.append(["Alabama", "Arkansas", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", "Michigan", "Minnesota", "Mississippi", "Missouri", "Nebraska", "North Dakota", "Ohio", "Oklahoma", "Pennsylvania", "South Dakota", "Tennessee", "Texas", "Vermont", "West Virginia", "Wisconsin", "Puerto Rico"])
        elif cur_dataset == 'urban':
          states.append(["California ","New Jersey ","Nevada ","Puerto Rico ","Massachusetts ","Hawaii ","Florida","Rhode Island ","Utah ","Arizona ","Illinois ","Connecticut ","New York ","Maryland ","Colorado ","Texas ","Washington ","Delaware ","Oregon "])
        elif cur_dataset == 'rural':
          states.append(["Pennsylvania", "Ohio", "New Mexico", "Virginia", "Georgia", "Michigan", "Kansas", "Minnesota", "Louisiana", "Nebraska", "Indiana", "Idaho", "Missouri", "Wisconsin", "Tennessee", "South Carolina", "Oklahoma", "North Carolina", "Alaska", "Wyoming", "Iowa", "New Hampshire", "North Dakota", "Alabama", "Kentucky", "South Dakota", "Arkansas", "Montana", "Mississippi", "West Virginia", "Vermont", "Maine", ])
        elif cur_dataset == 'north':
          states.append(["Washington","Oregon","Idaho","Montana","Wyoming","North Dakota","South Dakota","Nebraska","Minnesota","Iowa","Wisconsin","Illinois","Michigan","Indiana","Ohio","Pennslyvania","New York","Vermont","New Hampshire","Maine","Massasuchets","Conniticet","New Jersey"])
        elif cur_dataset == 'south':
          states.append(["California","Nevada","Utah","Arizona","New Mexico","Colorado","Texas","Oklahoma","Kansas","Arkansas","Missouri","Louisana","Missisipi","Alabama","Tenesee","Kentucky","Georgia","Florida","South Carolina","North Carolina","Virginia","West Virginia","Delaware","Maryland"])
        elif cur_dataset == 'all':
          states.append(["California","Nevada","Utah","Arizona","New Mexico","Colorado","Texas","Oklahoma","Kansas","Arkansas","Missouri","Louisana","Missisipi","Alabama","Tenesee","Kentucky","Georgia","Florida","South Carolina","North Carolina","Virginia","West Virginia","Delaware","Maryland","Washington","Oregon","Idaho","Montana","Wyoming","North Dakota","South Dakota","Nebraska","Minnesota","Iowa","Wisconsin","Illinois","Michigan","Indiana","Ohio","Pennslyvania","New York","Vermont","New Hampshire","Maine","Massasuchets","Conniticet","New Jersey"])
        else: 
          states.append(cur_dataset)
    return states

  def __adult_filter(self,data):
    """Mimic the filters in place for Adult data.
    Adult documentation notes: Extraction was done by Barry Becker from
    the 1994 Census database. A set of reasonably clean records was extracted
    using the following conditions:
    ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))

    Taken from the Folkstables Library.
    """
    df = data
    df = df[df['AGEP'] > 16]
    df = df[df['PINCP'] > 100]
    df = df[df['WKHP'] > 0]
    df = df[df['PWGTP'] >= 1]
    return df
  
  def __transform_labels_into_binary(self,threshold,ground_truth_labels):
    """
        before transformation, label = true income in dollar
        after transformation, label = either 1 if >threshold or 0 if <threshold.
    """
    if threshold == 'Average Income':
      threshold = np.mean(ground_truth_labels)
      ground_truth_labels = ground_truth_labels > threshold
    else:
      ground_truth_labels = ground_truth_labels > threshold
    return ground_truth_labels

  def get_income_data_with_track(self,states,years,input_threshold=50000,group_non_white=False,horizon='1-Year',survey='person',return_original_income=False):
    """
    Inputs:
    states : list of strings (states)

    years : list of non-negative numbers in the range of 2014 - 2018 (years)

    threshold : non-negative number
    default value: 50000
    The threshold used for the ACIncome task. (Every data sample (income) above the threshold belongs to class 1 and every data sample (income) below the threshold belongs to class 0)

    group_non_white: Boolean
    default value: False
    Whether every non-white race should be relabeled as "Non-White" or not.
      If False: 9 Unique Race Values (1,2,3,4,5,6,7,8,9)
      If True:  2 Unique Race Values (-1,1)


    horizon='1-Year'
    Not relevant for a typical user, used as input for the folkstable library.

    survey='person'
    Not relevant for a typical user, used as input for the folkstable library.

    return_original_income=False
    If set to True will addionaly return an income list, which contains the original income of each person. 
    This can be used for calculating the average yearly income etc.


    Returns:
    features : numpy array of arrays (the values of the features of each data sample, here 10 features)
    labels : numpy array of numbers (The class label of the data sample, see threshold)
    group : numpy array of numbers (The race of the data sample, see group_non_white)
    track_list_year : numpy array of non-negative numbers in the range of 2014-2018 (years)
    track_list_state : numpy array of strings (states)
    if return_original_income==True
    income_list : numpy array of numbers (The actual income of each person in US Dollars)
    """
    for i in range(len(years)):
      years[i] = str(years[i])

    features = np.zeros((0,10))
    labels = np.zeros((0))
    group = np.zeros((0))
    track_list_year = []
    track_list_state = []
    income_list = np.zeros((0))

    #Check if Threshold is an string keyword or an number.

    supported_threshold_keywords = ['Average Income']

    if type(input_threshold) == str:
      assert input_threshold in supported_threshold_keywords
      



    states = self.__get_state_code(states)

    IncomeProblem_own = BasicProblem(
        features=[
            'AGEP',
            'COW',
            'SCHL',
            'MAR',
            'OCCP',
            'POBP',
            'RELP',
            'WKHP',
            'SEX',
            'RAC1P',
        ],
        target='PINCP',
        #target_transform=lambda x: x > threshold,
        group='RAC1P',
        preprocess=self.__adult_filter,
        postprocess=lambda x: np.nan_to_num(x, -1),
      )

    for year in years:
      for state in states:
        data_source = ACSDataSource(survey_year=year, horizon=horizon, survey=survey)
        orig_data = data_source.get_data(states=[state], download=True)
        orig_features, orig_labels, orig_group = IncomeProblem_own.df_to_numpy(orig_data)

        if group_non_white==True:
          #1 stays 1 and everything else becomes -1
          orig_features[:,9] = 2*(orig_features[:,9]==1) -1
          orig_group = 2*orig_group[orig_group==1] -1

        
        if return_original_income:
          income_list = np.concatenate((income_list, orig_labels))
          self.__transform_labels_into_binary(input_threshold,orig_labels)
        features = np.concatenate((features, orig_features))
        labels = np.concatenate((labels, orig_labels))
        group = np.concatenate((group, orig_group))

        for i in range(len(orig_group)):
          track_list_year.append(int(year))
          track_list_state.append(state)

    #Transform the track list into numpy arrays to enable better sorting/searching
    track_list_state = np.array(track_list_state,dtype=np.str_)
    track_list_year = np.array(track_list_year,dtype=np.int32)

    bonus_information = {
        'group' : group,
        'track_list_year' : track_list_year,
        'track_list_state' : track_list_state,
    }


    if return_original_income:
      bonus_information['income_list'] = income_list
    #Else
    self.numpy_data_array = features
    self.ground_truth_labels = labels
    self.bonus_information = bonus_information
    
    return self.numpy_data_array,self.ground_truth_labels,self.bonus_information