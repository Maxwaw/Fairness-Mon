{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metrics-visualization.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iCOeFoTHETrW",
        "6fpgLf10qCtf",
        "xVXCYSLUBol0",
        "GjbQqy63Fbxk",
        "RYdtxRAqzTwW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìä **Metric Visualization Notebook**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cjgn3hkpI_62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, a visualization framework is developed which can dynamically generate plots from given parameters by displaying related data from metric files. Furthermore, a dashboard is created which serves as an interface for the framework and allows the user to easily explore our collected metrics.\n",
        "\n",
        "The purpose of this notebook is to allow students and researchers of AI fairness to explore how changes in temporal and/or spatial context can affect the fairness and performance of a model, as measured by various metrics. [Recent research has shown](https://arxiv.org/abs/2206.11436) that the interpretation of the fairness of classification models and datasets drastically depends on such context.\n",
        "\n",
        "<br></br>\n",
        "\n",
        "‚¨áÔ∏è To start, please <font color=\"orange\">run this cell</font>.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AsZUho6ND1qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we need a newer matplotlib version for some features\n",
        "!pip install -q -U matplotlib\n",
        "\n",
        "# restart the runtime for the update to take effect\n",
        "exit()"
      ],
      "metadata": {
        "id": "4hPXg_2Y8_Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colab will tell you that your runtime just crashed. **That is completely normal**. After Colab automatically restarted your session, you can execute the setup by <font color='orange'>clicking on the arrow on the very left</font> of the collapsed cells below:"
      ],
      "metadata": {
        "id": "egyG26GZ2XIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏è **Setup**\n",
        "\n",
        "‚¨áÔ∏è <font color='orange'>Click on the arrow below to run the setup!</font>"
      ],
      "metadata": {
        "id": "iCOeFoTHETrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to download the metrics. They are currently located in a .zip-file on Google Drive, which we will download with the `gdown` command. If you are interested in adding any metrics, you can add them to the unzipped directory or create a new .zip-file and download it with `gdown` here."
      ],
      "metadata": {
        "id": "-DPsYpk4n_ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download \"results.zip\" from drive and unzip\n",
        "%%capture\n",
        "!rm -r results\n",
        "!gdown 1R397q9bdk7--M6B1xFJDDfSJ5rVLkmfu\n",
        "!unzip results.zip\n",
        "!rm results.zip"
      ],
      "metadata": {
        "id": "lKqBkQscZ9jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we're creating some constants that will be useful through the entire notebook."
      ],
      "metadata": {
        "id": "8wCK-pjboM-s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uENxiPjgDx9w"
      },
      "outputs": [],
      "source": [
        "# constants\n",
        "\n",
        "# directory containing csv files with metrics\n",
        "METRICS_DIR = \"/content/results\"\n",
        "\n",
        "# style to use for matplotlib plots, affects font sizes etc.\n",
        "PLOT_STYLE = \"poster\"\n",
        "\n",
        "# whether to save PNG and PDF plots\n",
        "SAVE_PNG = False\n",
        "SAVE_PDF = False\n",
        "\n",
        "# output directory for all plots\n",
        "OUTPUT_DIR_PNG = f\"/content/plots_png/\"\n",
        "OUTPUT_DIR_PDF = f\"/content/plots_pdf/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will finish our initial environment setup by importing all necessary packages, setting some global settings and creating directories for our plots."
      ],
      "metadata": {
        "id": "vci_VEv7oWsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "from ctypes import ArgumentError\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick\n",
        "import seaborn as sns\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Layout, AppLayout, VBox, HBox, Label, HTML, Output, Text\n",
        "from IPython.display import display, clear_output"
      ],
      "metadata": {
        "id": "xYdmlVhnE-4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# global pandas settings\n",
        "pd.set_option('display.expand_frame_repr', False)  # don't print dataframes across multiple lines\n",
        "pd.set_option('display.max_rows', None)  # display all rows when printing dataframes\n",
        "pd.options.mode.chained_assignment = None  # suppress warning for creating columns from two other cols\n",
        "\n",
        "# global matplotlib/seaborn settings\n",
        "plt.style.use([\"seaborn-darkgrid\", f\"seaborn-{PLOT_STYLE}\"])\n",
        "plt.rcParams[\"figure.autolayout\"] = True  # automatically choose plot layouts\n",
        "sns.set_style('darkgrid',\n",
        "              {'legend.frameon': True})"
      ],
      "metadata": {
        "id": "tgCcLgucEe41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (re-)create directories for plots\n",
        "\n",
        "# whether to remove directories containing plots (WARNING, all files will be lost)\n",
        "REMOVE_DIRECTORIES = False\n",
        "\n",
        "if REMOVE_DIRECTORIES:\n",
        "    import shutil\n",
        "    shutil.rmtree(OUTPUT_DIR_PNG, ignore_errors=True)\n",
        "    shutil.rmtree(OUTPUT_DIR_PDF, ignore_errors=True)\n",
        "\n",
        "# create directories in which to store plots in (if they don't already exist)\n",
        "Path(OUTPUT_DIR_PNG).mkdir(parents=True, exist_ok=True)\n",
        "Path(OUTPUT_DIR_PDF).mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "Xp0F9WBIGqhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading\n",
        "\n",
        "In this section, we will load all metrics from their respective .csv-file. All metrics are then stored in a dataframe that will be the basis for our visualizations."
      ],
      "metadata": {
        "id": "6fpgLf10qCtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a function which extracts all important parameters from the metric file name, since that is the easiest way to get them. This function might seem complex but it only does this simple task."
      ],
      "metadata": {
        "id": "rxvnvIS5pEkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_path_parts(filename):\n",
        "    \"\"\"\n",
        "    given filename with information,\n",
        "    return model_type, dataset_train, years_train, threshold_train, dataset_test, years_test, threshold_test, group_non_white\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_parts = filename.split('_')  # split model name into parts\n",
        "\n",
        "    # 1. remove \"metrics\"\n",
        "    if model_name_parts[0] == \"metrics\": model_name_parts.pop(0)\n",
        "\n",
        "    # 2. get model_type\n",
        "    # 2.1 if \"adv\", use special handling\n",
        "    if model_name_parts[0] == \"adv\":\n",
        "        model_name_parts.pop(0)  # pop \"adv\"\n",
        "        model_name_parts.pop(0)  # pop \"deb\"\n",
        "        model_type = f\"AdversarialDebiasing ({model_name_parts.pop(0)}ed)\"  # biased/debiased\n",
        "    else:\n",
        "        model_type = model_name_parts.pop(0)\n",
        "        model_type = model_type[0].upper() + model_type[1:]\n",
        "    \n",
        "    # 3. get train dataset\n",
        "    dataset_train = \"\"\n",
        "    years_train = \"\"\n",
        "    threshold_train = \"\"\n",
        "\n",
        "    while True:\n",
        "        tmp = model_name_parts.pop(0)\n",
        "\n",
        "        if all(i.isdigit() for i in tmp) and len(tmp)==5:\n",
        "            # threshold\n",
        "            threshold_train = tmp\n",
        "            break\n",
        "        elif any(i.isdigit() for i in tmp):\n",
        "            # years\n",
        "            years_train += tmp\n",
        "        else:\n",
        "            # dataset name\n",
        "            dataset_train += tmp\n",
        "\n",
        "    assert dataset_train != \"\" and years_train != \"\" and threshold_train != \"\",\\\n",
        "            \"Error parsing filename, could not find dataset_train, years_train or threshold_train\"\n",
        "\n",
        "    # 4. get test dataset\n",
        "    dataset_test = \"\"\n",
        "    years_test = \"\"\n",
        "    threshold_test = \"\"\n",
        "\n",
        "    while True:\n",
        "        tmp = model_name_parts.pop(0)\n",
        "\n",
        "        if all(i.isdigit() for i in tmp) and len(tmp)==5:\n",
        "            # threshold\n",
        "            threshold_test = tmp\n",
        "            break\n",
        "        elif any(i.isdigit() for i in tmp):\n",
        "            # years\n",
        "            years_test += tmp\n",
        "        else:\n",
        "            # dataset name\n",
        "            dataset_test += tmp\n",
        "\n",
        "    assert dataset_test != \"\" and years_test != \"\" and threshold_test != \"\",\\\n",
        "            \"Error parsing filename, could not find dataset_test, years_test or threshold_test\"\n",
        "\n",
        "    # 5. get group_non_white\n",
        "    if \"True\" in model_name_parts[-1]:\n",
        "        # true\n",
        "        group_non_white = True\n",
        "    elif \"False\" in model_name_parts[-1]:\n",
        "        # false\n",
        "        group_non_white = False\n",
        "    else:\n",
        "        # error\n",
        "        raise RuntimeError('Could not find value for group_non_white')\n",
        "\n",
        "    years_train = years_train.replace(\" \", \", \")\n",
        "    years_test = years_test.replace(\" \", \", \")\n",
        "\n",
        "    return [model_type, dataset_train, years_train, threshold_train, dataset_test, years_test, threshold_test, group_non_white]"
      ],
      "metadata": {
        "id": "t8OHQxVaeUPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parse metrics directory and put all metrics into one large dataframe with cols:\n",
        "#   model_type, dataset_train, dataset_test, threshold_train, threshold_test,\n",
        "#   years_train, years_test, group_non_white, *METRICS\n",
        "\n",
        "# number of directories in METRICS_DIR path, we need this to correctly parse\n",
        "# directory names for different METRICS_DIR values\n",
        "irrelevant_dirs_num = len(METRICS_DIR.split(\"/\"))-1\n",
        "\n",
        "# list of all metrics\n",
        "metrics = []\n",
        "\n",
        "df_all = pd.DataFrame()\n",
        "\n",
        "for subdir, dirs, files in os.walk(METRICS_DIR):\n",
        "    if \"Old\" in subdir: continue  # skip old metrics\n",
        "    for file in files:\n",
        "        if not file.endswith(\".csv\"): continue  # skip non-metric files\n",
        "        if file.endswith(\").csv\"): continue  # skip duplicate files\n",
        "\n",
        "        model_type, dataset_train, years_train, threshold_train, dataset_test, years_test, threshold_test, group_non_white = parse_path_parts(file)\n",
        "\n",
        "        full_path = os.path.join(subdir, file)\n",
        "        df = pd.read_csv(full_path)\n",
        "        df.drop('model_name', axis=1, inplace=True)\n",
        "\n",
        "        # add columns for values parsed from filename\n",
        "        df[\"model_type\"] = model_type\n",
        "        df[\"dataset_train\"] = dataset_train\n",
        "        df[\"years_train\"] = years_train\n",
        "        df[\"threshold_train\"] = threshold_train\n",
        "        df[\"dataset_test\"] = dataset_test\n",
        "        df[\"years_test\"] = years_test\n",
        "        df[\"threshold_test\"] = threshold_test\n",
        "        df[\"group_non_white\"] = group_non_white\n",
        "\n",
        "        df_all = pd.concat([df_all, df], ignore_index=True)\n",
        "\n",
        "# calculate f1 score and add to dataframe\n",
        "df_all[\"f1_score\"] = 2 * (df_all[\"precision\"]*df_all[\"recall\"])/(df_all[\"precision\"]+df_all[\"recall\"])\n",
        "\n",
        "# sort columns alphabetically\n",
        "df_all = df_all.reindex(sorted(df_all.columns), axis=1)\n",
        "\n",
        "# move filename columns to front\n",
        "df_all.insert(0, 'group_non_white', df_all.pop('group_non_white'))\n",
        "df_all.insert(0, 'threshold_test', df_all.pop('threshold_test'))\n",
        "df_all.insert(0, 'years_test', df_all.pop('years_test'))\n",
        "df_all.insert(0, 'dataset_test', df_all.pop('dataset_test'))\n",
        "df_all.insert(0, 'threshold_train', df_all.pop('threshold_train'))\n",
        "df_all.insert(0, 'years_train', df_all.pop('years_train'))\n",
        "df_all.insert(0, 'dataset_train', df_all.pop('dataset_train'))\n",
        "df_all.insert(0, 'model_type', df_all.pop('model_type'))\n",
        "\n",
        "df_all"
      ],
      "metadata": {
        "id": "d0xqPSBhG4b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save all metrics to csv with current timestamp\n",
        "dt_string = datetime.now().strftime(\"%d%m%Y-%H:%M\")\n",
        "df_all.to_csv(f\"all_metrics_{dt_string}.csv\", encoding='utf-8', index=False)"
      ],
      "metadata": {
        "id": "PESqUIDyqKML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop columns where all values are NaN\n",
        "df_all = df_all.loc[:, df_all.notna().all()]\n",
        "\n",
        "# drop \"object\" type columns (confusion matrices etc.) as we can't plot them\n",
        "df_all.drop(columns=df_all.iloc[:, 8:].select_dtypes(include=[\"object\"]).columns,\n",
        "            inplace=True)\n",
        "\n",
        "# replace space in AdversarialDebiasing model names with newline\n",
        "df_all.loc[:, 'model_type'] = df_all.model_type.apply(lambda x: x.replace(\" \", \"\\n\"))"
      ],
      "metadata": {
        "id": "MLF3pVry-Kva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizations"
      ],
      "metadata": {
        "id": "tmwX4P-xqGRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have all the relevant data, it is time to work on the visualizations."
      ],
      "metadata": {
        "id": "DsBQIlwqpWl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Exploration\n",
        "\n",
        "Let's have a look at our collected metrics and our data distribution."
      ],
      "metadata": {
        "id": "xVXCYSLUBol0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We have the following metrics:**\n",
        "\n",
        "* ***abroca***\n",
        "* accuracy\n",
        "* average_abs_odds_difference\n",
        "* average_odds_difference\n",
        "* base_rate\n",
        "\n",
        ">* between_all_groups_coefficient_of_variation\n",
        ">* between_all_groups_generalized_entropy_index\n",
        ">* between_all_groups_theil_index\n",
        ">* between_group_coefficient_of_variation\n",
        ">* between_group_generalized_entropy_index\n",
        ">* between_group_theil_index\n",
        "\n",
        "* binary_confusion_matrix\n",
        "* coefficient_of_variation\n",
        "* consistency\n",
        "* differential_fairness_bias_amplification\n",
        "* ***disparate_impact***\n",
        "* ***equal_opportunity_difference*** (alias of true_positive_rate_difference)\n",
        "\n",
        ">* error_rate\n",
        ">* error_rate_difference\n",
        ">* error_rate_ratio\n",
        "\n",
        ">* false_discovery_rate\n",
        ">* false_discovery_rate_difference\n",
        ">* false_discovery_rate_ratio\n",
        "\n",
        ">* false_negative_rate\n",
        ">* false_negative_rate_difference\n",
        ">* false_negative_rate_ratio\n",
        "\n",
        ">* false_omission_rate\n",
        ">* false_omission_rate_difference\n",
        ">* false_omission_rate_ratio\n",
        "\n",
        ">* false_positive_rate\n",
        ">* false_positive_rate_difference\n",
        ">* false_positive_rate_ratio\n",
        "\n",
        ">* generalized_binary_confusion_matrix\n",
        ">* generalized_entropy_index\n",
        ">* generalized_false_negative_rate\n",
        ">* generalized_false_positive_rate\n",
        ">* generalized_true_negative_rate\n",
        ">* generalized_true_positive_rate\n",
        "\n",
        "* mean_difference (alias of statistical_parity_difference)\n",
        "* negative_predictive_value\n",
        "\n",
        ">* num_false_negatives\n",
        ">* num_false_positives\n",
        ">* num_generalized_false_negatives\n",
        ">* num_generalized_false_positives\n",
        ">* num_generalized_true_negatives\n",
        ">* num_generalized_true_positives\n",
        ">* num_instances\n",
        ">* num_negatives\n",
        ">* num_positives\n",
        ">* num_pred_negatives\n",
        ">* num_pred_positives\n",
        ">* num_true_negatives\n",
        ">* num_true_positives\n",
        "\n",
        "* performance_measures\n",
        "* positive_predictive_value\n",
        "* power (alias of num_true_positives)\n",
        "* precision (alias of positive_predictive_value)\n",
        "* recall (alias of true_positive_rate)\n",
        "* selection_rate\n",
        "* sensitivity (alias of true_positive_rate)\n",
        "* smoothed_empirical_differential_fairness\n",
        "* specificity (alias of true_negative_rate)\n",
        "* statistical_parity_difference\n",
        "* theil_index\n",
        "\n",
        ">* true_negative_rate\n",
        ">* true_positive_rate\n",
        ">* true_positive_rate_difference"
      ],
      "metadata": {
        "id": "rE8L3qsV_PKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get overview over distribution of test datasets\n",
        "print(\"Distribution of Test Datasets:\")\n",
        "print(df_all.groupby([\"dataset_test\", \"years_test\", \"threshold_test\"]).size())\n",
        "\n",
        "# get overview over distribution of train datasets\n",
        "print(\"-\"*60 + \"\\nDistribution of Train Datasets:\")\n",
        "print(df_all.groupby([\"dataset_train\", \"years_train\", \"threshold_train\"]).size())"
      ],
      "metadata": {
        "id": "iwlyqUTDBHOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a helper function which allows us to easily filter the dataframe for the values we are interested in."
      ],
      "metadata": {
        "id": "gaHNB_nDpx79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_metrics(set_type, dataset, years, threshold):\n",
        "    if set_type not in {\"train\", \"test\"}: raise ArgumentError(\"Unknown set_type, choose either 'train' or 'test'.\")\n",
        "    return df_all.loc[(df_all[f\"dataset_{set_type}\"]==dataset) &\n",
        "           (df_all[f\"years_{set_type}\"]==years) &\n",
        "           (df_all[f\"threshold_{set_type}\"]==threshold)]"
      ],
      "metadata": {
        "id": "iENSA6jJqHFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how we can filter the dataframe to get only metrics from models `test`ed on `east coast geo` in the year `2018` with a threshold of `30000`."
      ],
      "metadata": {
        "id": "ceuceGc7uueA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of how to filter the dataframe to get only metrics from a specific test set\n",
        "set_metrics(\"test\", \"east coast geo\", \"2018\", \"30000\")"
      ],
      "metadata": {
        "id": "4Hs1Iab7mznL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting\n",
        "\n",
        "In this subsection, the actual visualization function is defined, which is the heart of this notebook."
      ],
      "metadata": {
        "id": "R53e1LBXBsJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper Functions\n",
        "\n",
        "Our visualization relies on helper functions for many tasks."
      ],
      "metadata": {
        "id": "GjbQqy63Fbxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function for bar chart labels\n",
        "def autolabel(rects, use_ints=False):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        annotation = str(int(height)) if use_ints else f'{height:.3f}'\n",
        "        ax.annotate(annotation,\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    ha='center',\n",
        "                    va='bottom',\n",
        "                    fontsize=plt.rcParams['axes.labelsize'])"
      ],
      "metadata": {
        "id": "wLJ1YmO0FdE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_name_to_pos(dataset_name, ignore_dataset=False, ignore_year=False):\n",
        "    \"\"\"\n",
        "    Assign a given dataset name an integer, such that one can sort datasets in a\n",
        "    custom order.\n",
        "\n",
        "    The custom order is:\n",
        "    1. East Coast Geo\n",
        "    2. West Coast Geo\n",
        "    3. Urban\n",
        "    4. Rural\n",
        "\n",
        "    Inside datasets, it will be sorted by year.\n",
        "    \"\"\"\n",
        "\n",
        "    val = -1\n",
        "\n",
        "    if not ignore_dataset:\n",
        "        if \"east\" in dataset_name:\n",
        "            val = 0\n",
        "        elif \"west\" in dataset_name:\n",
        "            val = 6\n",
        "        elif \"urban\" in dataset_name:\n",
        "            val = 12\n",
        "        elif \"rural\" in dataset_name:\n",
        "            val = 18\n",
        "        else:\n",
        "            raise ArgumentError(f\"did not recognize dataset name: {dataset_name}\")\n",
        "\n",
        "    if not ignore_year:\n",
        "        if \"2014\" in dataset_name:\n",
        "            val += 1\n",
        "        elif \"2015\" in dataset_name:\n",
        "            val += 2\n",
        "        elif \"2016\" in dataset_name:\n",
        "            val += 3\n",
        "        elif \"2017\" in dataset_name:\n",
        "            val += 4\n",
        "        elif \"2018\" in dataset_name:\n",
        "            val += 5\n",
        "        else:\n",
        "            raise ArgumentError(f\"did not recognize year in dataset name: {dataset_name}\")\n",
        "\n",
        "    return val"
      ],
      "metadata": {
        "id": "0Hcdet3ckzVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def colormap_from_dataset_names(dataset_names):\n",
        "    \"\"\"\n",
        "    Given a list of dataset names, create a colormap that assigns each dataset name\n",
        "    a color.\n",
        "\n",
        "    The same datasets with different years should receive similar colors.\n",
        "    \"\"\"\n",
        "\n",
        "    cm_east = plt.cm.Blues_r\n",
        "    cm_west = plt.cm.Oranges_r\n",
        "    cm_urban = plt.cm.Greens_r\n",
        "    cm_rural = plt.cm.Purples_r\n",
        "\n",
        "    count_east = 0\n",
        "    count_west = 0\n",
        "    count_urban = 0\n",
        "    count_rural = 0\n",
        "\n",
        "    for name in dataset_names:\n",
        "        if \"east\" in name:\n",
        "            count_east += 1\n",
        "        elif \"west\" in name:\n",
        "            count_west += 1\n",
        "        elif \"urban\" in name:\n",
        "            count_urban += 1\n",
        "        elif \"rural\" in name:\n",
        "            count_rural += 1\n",
        "        else:\n",
        "            raise ArgumentError(f\"did not recognize dataset name: {name}\")\n",
        "\n",
        "    cm = []\n",
        "    cm_linspace = np.linspace(.2, .8, 5)\n",
        "\n",
        "    cm.extend(list(cm_east(cm_linspace))[:count_east])\n",
        "    cm.extend(list(cm_west(cm_linspace))[:count_west])\n",
        "    cm.extend(list(cm_urban(cm_linspace))[:count_urban])\n",
        "    cm.extend(list(cm_rural(cm_linspace))[:count_rural])\n",
        "\n",
        "    return cm"
      ],
      "metadata": {
        "id": "cX9kVrCOnkJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def refilter_metrics(vis_metrics, filters, iter_type):\n",
        "    filter_mt, filter_ds, filter_yr = filters[0]\n",
        "\n",
        "    if filters[1]:\n",
        "        # exclusively include filtered metrics\n",
        "        if filter_mt is not None:\n",
        "            if type(filter_mt) is list:\n",
        "                for f_mt in filter_mt:\n",
        "                    vis_metrics = vis_metrics.loc[(vis_metrics[f\"model_type\"]==f_mt)]\n",
        "            else:\n",
        "                vis_metrics = vis_metrics.loc[(vis_metrics[f\"model_type\"]==filter_mt)]\n",
        "        if filter_ds is not None:\n",
        "            if type(filter_ds) is list:\n",
        "                for f_ds in filter_ds:\n",
        "                    vis_metrics = vis_metrics.loc[(vis_metrics[f\"model_type\"]==f_ds)]\n",
        "            else:\n",
        "                vis_metrics = vis_metrics.loc[(vis_metrics[f\"dataset_{iter_type}\"]==filter_ds)]\n",
        "        if filter_yr is not None:\n",
        "            if type(filter_yr) is list:\n",
        "                for f_yr in filter_yr:\n",
        "                    vis_metrics = vis_metrics.loc[(vis_metrics[f\"model_type\"]==f_yr)]\n",
        "            else:\n",
        "                vis_metrics = vis_metrics.loc[(vis_metrics[f\"years_{iter_type}\"]==filter_yr)]\n",
        "    else:\n",
        "        # exclude filtered metrics\n",
        "        if filter_mt is not None:\n",
        "            if type(filter_mt) is list:\n",
        "                for f_mt in filter_mt:\n",
        "                    vis_metrics = vis_metrics.loc[(vis_metrics[f\"model_type\"]==f_mt)]\n",
        "            else:\n",
        "                vis_metrics = vis_metrics.loc[(vis_metrics[f\"model_type\"]!=filter_mt)]\n",
        "        if filter_ds is not None:\n",
        "            if type(filter_ds) is list:\n",
        "                for f_ds in filter_ds:\n",
        "                    vis_metrics = vis_metrics.loc[(vis_metrics[f\"model_type\"]==f_ds)]\n",
        "            else:\n",
        "                vis_metrics = vis_metrics.loc[(vis_metrics[f\"dataset_{iter_type}\"]!=filter_ds)]\n",
        "        if filter_yr is not None:\n",
        "            if type(filter_yr) is list:\n",
        "                for f_yr in filter_yr:\n",
        "                    vis_metrics = vis_metrics.loc[(vis_metrics[f\"model_type\"]==f_yr)]\n",
        "            else:\n",
        "                vis_metrics = vis_metrics.loc[(vis_metrics[f\"years_{iter_type}\"]!=filter_yr)]\n",
        "\n",
        "    return vis_metrics"
      ],
      "metadata": {
        "id": "w0hjhFFhTcRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ranges of each metric\n",
        "metrics_range_zero_one = ['accuracy', 'average_abs_odds_difference', 'base_rate',\n",
        " 'error_rate', 'f1_score', 'false_discovery_rate', 'false_negative_rate',\n",
        " 'false_omission_rate', 'false_positive_rate', 'negative_predictive_value',\n",
        " 'positive_predictive_value', 'precision', 'recall', 'selection_rate',\n",
        " 'sensitivity', 'specificity', 'theil_index', 'true_negative_rate',\n",
        " 'true_positive_rate']\n",
        "\n",
        "pres_metrics_threshold = [\"accuracy\", \"abroca\", \"f1_score\", \"disparate_impact\"]\n",
        "\n",
        "def get_limits_for_metric(metric):\n",
        "    if metric in metrics_range_zero_one:\n",
        "        return [0, 1]\n",
        "    elif \"ratio\" in metric:\n",
        "        return [0-get_base_for_metric(metric), None]\n",
        "    return [None, None]\n",
        "\n",
        "# give baseline value for given metric\n",
        "def get_base_for_metric(metric):\n",
        "    if \"ratio\" in metric or metric==\"disparate_impact\":\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# interpretation help for each metric\n",
        "metrics_hints = {\"abroca\": \"0 is best\",\n",
        "                 \"accuracy\": \"higher is better\",\n",
        "                 \"f1_score\": \"higher is better\",\n",
        "                 \"disparate_impact\": \"1 is best\"}\n",
        "\n",
        "def get_hint_for_metric(metric):\n",
        "    if metric in metrics_hints:\n",
        "        return f\" ({metrics_hints[metric]})\"\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "HiyMOOh5tn-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main Function\n",
        "\n",
        "Here we define the main visualizaion function which takes a series of parameters and automatically creates a corresponding grouped bar chart."
      ],
      "metadata": {
        "id": "FEtygCI9Fdh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_experiment(eval_type, eval_dataset, eval_years, threshold, metric,\n",
        "                    filters=(None, True), forced_legend_loc=None, forced_filename=None,\n",
        "                    save_png=SAVE_PNG, save_pdf=SAVE_PDF):\n",
        "    \"\"\"\n",
        "    Plot a grouped bar chart using metrics from experiment with given parameters.\n",
        "\n",
        "        Parameters:\n",
        "            eval_type (str): Either 'train' or 'test'\n",
        "                             if 'test':  compare different trainings on the same test dataset\n",
        "                             if 'train': compare the same training on different test datasets \n",
        "            eval_dataset (str): Dataset to use for evaluation\n",
        "            eval_years (str): Years to use for evaluation\n",
        "            threshold (str): Threshold to use for evaluation\n",
        "            metric (str): Metric to use for evaluation\n",
        "            filters ((list[(str or list(str)), (str or list(str)), (str or list(str))], bool), default (None, True)): \n",
        "                Additional filters if you don't want to include all existings metrics\n",
        "                in your plot. Order is ([model_type, dataset, years], include).\n",
        "                \"include\" denotes whether filtered plots should be exclusively included\n",
        "                (True) or excluded (False).\n",
        "                If None, include everything.\n",
        "            forced_legend_loc (str, default None): Position to force legend in, if None, choose 'best'.\n",
        "                                            This can be helpful if the legend overlaps with the graph.\n",
        "            forced_filename (str, default None): Filename to use for saving, without file ending.\n",
        "                                                 If None, automatically determine from parameters.\n",
        "            save_png (bool, default SAVE_PNG): Whether to save plot as PNG\n",
        "            save_pdf (bool, default SAVE_PDF): Whether to save plot as PDF\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "    \"\"\"\n",
        "    # the inverse of eval_type; the type to iterate over\n",
        "    iter_type = \"train\" if eval_type==\"test\" else \"test\"\n",
        "\n",
        "    # get metrics from given parameters\n",
        "    vis_metrics = set_metrics(eval_type, eval_dataset, eval_years, threshold)\n",
        "\n",
        "    # if specified, only take filtered metrics\n",
        "    if filters[0] is not None:\n",
        "        vis_metrics = refilter_metrics(vis_metrics, filters, iter_type)\n",
        "            \n",
        "    # if no metrics were found, do not create visualization and output information\n",
        "    if len(vis_metrics)==0:\n",
        "        print(\"No metrics with given parameters were found. Please try different parameters or upload your metrics to the directory.\")\n",
        "        if filters is not None: print(\"Consider using less strict filters.\")\n",
        "        return\n",
        "\n",
        "    # create merged column from dataset and years\n",
        "    vis_metrics[f\"dataset_years_{iter_type}\"] = [f\"{ds} ({yr})\" for ds, yr in zip(vis_metrics[f\"dataset_{iter_type}\"], vis_metrics[f\"years_{iter_type}\"])]\n",
        "\n",
        "    # sort that column and use it as the order of our grouped bar chart\n",
        "    hue_order = list(dict.fromkeys(sorted(vis_metrics[f\"dataset_years_{iter_type}\"].to_list(),\n",
        "                                          key=dataset_name_to_pos)))\n",
        "\n",
        "    # create colormap which assigns each present dataset its respective color\n",
        "    cmap = colormap_from_dataset_names(hue_order)\n",
        "\n",
        "    # update rcParams with new color map and figure size \n",
        "    plt.rcParams['axes.prop_cycle'] = plt.cycler(color=cmap)\n",
        "    plt.rcParams['figure.figsize'] = 16, 9\n",
        "\n",
        "    # some metrics require a baseline other than 0, as they should be considered\n",
        "    # to be centered around a different value, e.g. 1\n",
        "    baseline = get_base_for_metric(metric)\n",
        "\n",
        "    # we need to subtract the baseline from the metrics because seaborn will always\n",
        "    # let the bars start from 0\n",
        "    vis_metrics[metric] -= baseline\n",
        "\n",
        "    # create the actual bar chart\n",
        "    ax = sns.barplot(x=\"model_type\",              \n",
        "                    y=metric,         \n",
        "                    hue=f\"dataset_years_{iter_type}\",\n",
        "                    data=vis_metrics,\n",
        "                    order=vis_metrics[\"model_type\"].sort_values().unique(),\n",
        "                    hue_order=hue_order,\n",
        "                    ci=None)\n",
        "\n",
        "    # add baseline to yticks to make them display the actual values\n",
        "    ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda x,_: f\"{x+baseline:g}\"))\n",
        "\n",
        "    # format plot to look better by doing some small things\n",
        "    metric_fancy = metric.replace(\"_\", \" \").title()\n",
        "    if metric == \"abroca\": metric_fancy = \"ABROCA\"\n",
        "\n",
        "    ds_fancy = eval_dataset.replace(\"_\", \" \").title()\n",
        "\n",
        "    # \"best\" sometimes does not work well, especially for the \"accuracy\" metric\n",
        "    legend_loc = \"lower left\" if (metric == \"accuracy\" or metric == \"f1_score\") else \"best\"\n",
        "    legend_loc = forced_legend_loc if forced_legend_loc is not None else legend_loc\n",
        "\n",
        "    # set chart title depending on eval_type to help the user understand the plot\n",
        "    if eval_type == \"test\":\n",
        "        title = f\"{metric_fancy} on {ds_fancy} ({eval_years}) with Threshold={int(threshold):,}\"\n",
        "    else:\n",
        "        title = f\"{metric_fancy} of Model trained on {ds_fancy} ({eval_years}) with Threshold={int(threshold):,}\"\n",
        "\n",
        "    # we defined a hint for the main metrics, we should add it to the yaxis label\n",
        "    metric_fancy = f\"{metric_fancy}{get_hint_for_metric(metric)}\"\n",
        "    ax.set_ylabel(metric_fancy)\n",
        "    ax.set_xlabel('Model')\n",
        "    ax.set_title(title)\n",
        "\n",
        "    if forced_legend_loc != \"none\":\n",
        "        ax.legend(title=f\"{iter_type.capitalize()}ing Dataset\",\n",
        "                    fancybox=True,\n",
        "                    title_fontsize=plt.rcParams['legend.fontsize'],\n",
        "                    frameon=True,\n",
        "                    loc=legend_loc,\n",
        "                    facecolor=\"w\")\n",
        "    else:\n",
        "        ax.legend_.remove()\n",
        "    \n",
        "    # set ylimits for current metric\n",
        "    ax.set_ylim(get_limits_for_metric(metric))\n",
        "\n",
        "    # update ylimit to leave some space if bars come from upper end of plot\n",
        "    if baseline==1 and ax.get_ylim()[1]==1-baseline: \n",
        "        ax.set_ylim(top=0.1)\n",
        "\n",
        "    if metric==\"abroca\" and ax.get_ylim()[0] == 0:\n",
        "        ax.set_ylim(bottom=ax.get_ylim()[1]*-.2)\n",
        "\n",
        "    # denote if any bars have positive/negative values, useful for baseline highlighting\n",
        "    any_positive = any([any([d>0 for d in c.datavalues]) for c in ax.containers])\n",
        "    any_negative = any([any([d<0 for d in c.datavalues]) for c in ax.containers])\n",
        "\n",
        "    # add horizontal line to highlight baseline, if != 0\n",
        "    if baseline != 0 or (any_positive and any_negative) or metric==\"abroca\": ax.axhline(0, color=\"darkgrey\")\n",
        "\n",
        "    # use integer format for integers, 3-digit float format for floats\n",
        "    fmt= \"{label:.0f}\" if all([all([d.is_integer() for d in c.datavalues]) for c in ax.containers]) else \"{label:.3f}\"\n",
        "\n",
        "    # add value label to each bar\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container,\n",
        "                    fontsize=plt.rcParams['xtick.labelsize'],\n",
        "                    fmt=fmt,\n",
        "                    labels=[fmt.format(label=x+baseline) for x in container.datavalues],\n",
        "                    padding=2)\n",
        "\n",
        "    # include a tight_layout() call for good measure\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # construct filename from parameters, if no forced_filename was given\n",
        "    filename = f\"{eval_type}_{eval_dataset}_{eval_years}_{threshold}_{metric}\"\n",
        "    filename = forced_filename if forced_filename is not None else filename\n",
        "\n",
        "    # save plots as png/pdf in respective directory\n",
        "    if save_png: plt.savefig(f\"{OUTPUT_DIR_PNG}{filename}.png\")\n",
        "    if save_pdf: plt.savefig(f\"{OUTPUT_DIR_PDF}{filename}.pdf\")\n",
        "\n",
        "    # return the plot\n",
        "    return ax"
      ],
      "metadata": {
        "id": "NtSP4tMBziVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example of how to use this function."
      ],
      "metadata": {
        "id": "_vnW6kh8uSDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot = plot_experiment(\n",
        "    eval_type    = \"train\",\n",
        "    eval_dataset = \"east coast geo\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"disparate_impact\")\n",
        "\n",
        "plt.show(plot)"
      ],
      "metadata": {
        "id": "j09fpzfcEtbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plots for the Presentation\n",
        "\n",
        "In this section, we create the plots for our presentation slides.\n",
        "\n",
        "Please keep in mind that the plots shown in our presentation slightly differ from the ones created here, since we used custom yaxis limits and color nuances for some plots to make them more accurate and easier to interpret.\n",
        "\n",
        "If you wish to take a look at them, we suggest you take a look at our [presentation slides](https://docs.google.com/presentation/d/1ET3pKml7Sgjx6Y7E3P3XhLVUYo0IEelsB4JrQ7U1ktc/edit?usp=sharing)."
      ],
      "metadata": {
        "id": "GQsYnWEIME_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Temporal vs. Spatial Context\n",
        "\n",
        ">Evaluate: West Coast (2018)\n",
        ">\n",
        ">Training: Everything Compared\n"
      ],
      "metadata": {
        "id": "3qTRAUf1McKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tvsc_abr = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"west coast geo\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"abroca\",\n",
        "    forced_legend_loc = \"lower center\",\n",
        "    forced_filename = \"tvsc_abr\",\n",
        "    save_png = True)\n",
        "\n",
        "plt.show(tvsc_abr)"
      ],
      "metadata": {
        "id": "GRTBR6uUMEnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tvsc_acc = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"west coast geo\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"accuracy\",\n",
        "    forced_filename = \"tvsc_acc\",\n",
        "    save_png = True)\n",
        "\n",
        "plt.show(tvsc_acc)"
      ],
      "metadata": {
        "id": "V--CQhjOXC_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tvsc_f1 = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"west coast geo\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"f1_score\",\n",
        "    forced_filename = \"tvsc_f1\",\n",
        "    save_png = True)\n",
        "\n",
        "plt.show(tvsc_f1)"
      ],
      "metadata": {
        "id": "94dvxudjXRkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tvsc_di = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"west coast geo\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"disparate_impact\",\n",
        "    forced_filename = \"tvsc_di\",\n",
        "    save_png = True)\n",
        "\n",
        "plt.show(tvsc_di)"
      ],
      "metadata": {
        "id": "9nM2mEehXUhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Temporal Context\n",
        "\n",
        ">Evaluate: Rural (2018)\n",
        ">\n",
        ">Training: Rural (2014) vs. Rural (2016) vs. Rural (2018) vs. Rural (2014, 2016) vs. Rural (2014, 2016, 2018)\n",
        "\n"
      ],
      "metadata": {
        "id": "_Apr9azBNBJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tc_abr = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"abroca\",\n",
        "    filters      = ([None, \"rural\", None], True),\n",
        "    forced_filename = \"tc_abr\",\n",
        "    forced_legend_loc = \"lower right\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(tc_abr)"
      ],
      "metadata": {
        "id": "M7mu0ia_Z0ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tc_acc = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"accuracy\",\n",
        "    filters      = ([None, \"rural\", None], True),\n",
        "    forced_filename = \"tc_acc\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(tc_acc)"
      ],
      "metadata": {
        "id": "UrQcRQYoZz_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tc_di = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"disparate_impact\",\n",
        "    filters      = ([None, \"rural\", None], True),\n",
        "    forced_filename = \"tc_di\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(tc_di)"
      ],
      "metadata": {
        "id": "OSQgVbRyZzdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tc_f1 = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"f1_score\",\n",
        "    filters      = ([None, \"rural\", None], True),\n",
        "    forced_filename = \"tc_f1\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(tc_f1)"
      ],
      "metadata": {
        "id": "ovHgFJWdNBJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Spatial Context\n",
        "\n",
        ">Evaluate: Rural (2016)\n",
        ">\n",
        ">Training: East Coast (2016) vs. West Coast (2016) vs. Rural (2016)\n"
      ],
      "metadata": {
        "id": "xWlP3ItnNDI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc_abr = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2016\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"abroca\",\n",
        "    filters      = ([None, None, \"2016\"], True),\n",
        "    forced_filename = \"sc_abr\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(sc_abr)"
      ],
      "metadata": {
        "id": "hr1WT5YkP6YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc_acc = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2016\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"accuracy\",\n",
        "    filters      = ([None, None, \"2016\"], True),\n",
        "    forced_filename = \"sc_acc\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(sc_acc)"
      ],
      "metadata": {
        "id": "P7avrOqIP6oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc_di = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2016\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"disparate_impact\",\n",
        "    filters      = ([None, None, \"2016\"], True),\n",
        "    forced_filename = \"sc_di\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(sc_di)"
      ],
      "metadata": {
        "id": "MUgh_BLbNDI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc_f1 = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2016\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"f1_score\",\n",
        "    filters      = ([None, None, \"2016\"], True),\n",
        "    forced_filename = \"sc_f1\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(sc_f1)"
      ],
      "metadata": {
        "id": "Z8ldMfjIP535"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Threshold Impact\n",
        "\n",
        ">Evaluate: Rural (2018, 30k Threshold), Rural (2018, 40k Threshold), Rural (2018, 50k Threshold), Rural (2018, 60k Threshold)\n",
        ">\n",
        ">Training: Rural (2018, 30k Threshold), Rural (2018, 40k Threshold), Rural (2018, 50k Threshold), Rural (2018, 60k Threshold)\n"
      ],
      "metadata": {
        "id": "WmolIzpyNEJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 30,000"
      ],
      "metadata": {
        "id": "WuJQIQnQVsqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "th_abr_30k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"30000\",\n",
        "    metric       = \"abroca\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_abr_30k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_abr_30k)"
      ],
      "metadata": {
        "id": "dw-Gd0P_Vsqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th_acc_30k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"30000\",\n",
        "    metric       = \"accuracy\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_acc_30k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_acc_30k)"
      ],
      "metadata": {
        "id": "8WZ8CKXZVsqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th_f1_30k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"30000\",\n",
        "    metric       = \"f1_score\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_f1_30k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_f1_30k)"
      ],
      "metadata": {
        "id": "4TzVH5YCVsqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th_di_30k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"30000\",\n",
        "    metric       = \"disparate_impact\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_di_30k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_di_30k)"
      ],
      "metadata": {
        "id": "UbGy072gVsqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 40,000"
      ],
      "metadata": {
        "id": "St-1dgr6VnPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "th_abr_40k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"40000\",\n",
        "    metric       = \"abroca\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_abr_40k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_abr_40k)"
      ],
      "metadata": {
        "id": "XoQPb9ejVI23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th_acc_40k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"40000\",\n",
        "    metric       = \"accuracy\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_acc_40k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_acc_40k)"
      ],
      "metadata": {
        "id": "V4ImpS_1VIrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th_f1_40k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"40000\",\n",
        "    metric       = \"f1_score\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_f1_40k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_f1_40k)"
      ],
      "metadata": {
        "id": "KF9QjIv4VIaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th_di_40k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"40000\",\n",
        "    metric       = \"disparate_impact\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_di_40k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_di_40k)"
      ],
      "metadata": {
        "id": "J-VQStSrNEJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 50,000"
      ],
      "metadata": {
        "id": "mJK7nSwjVu1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "th_abr_50k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"abroca\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_abr_50k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_abr_50k)"
      ],
      "metadata": {
        "id": "nELHfZT3Vu1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th_acc_50k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"accuracy\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_acc_50k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_acc_50k)"
      ],
      "metadata": {
        "id": "sT5yt0goVu1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th_f1_50k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"f1_score\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_f1_50k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_f1_50k)"
      ],
      "metadata": {
        "id": "MeoB42mcVu1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th_di_50k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"50000\",\n",
        "    metric       = \"disparate_impact\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_di_50k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_di_50k)"
      ],
      "metadata": {
        "id": "RHOClsqVVu1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### 60,000"
      ],
      "metadata": {
        "id": "1_8Z-q-QucjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "th_abr_60k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"60000\",\n",
        "    metric       = \"abroca\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_abr_60k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_abr_60k)"
      ],
      "metadata": {
        "id": "YjAPhaRtucjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th_acc_60k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"60000\",\n",
        "    metric       = \"accuracy\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_acc_60k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_acc_60k)"
      ],
      "metadata": {
        "id": "4JIfOOWhucjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th_f1_60k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"60000\",\n",
        "    metric       = \"f1_score\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_f1_60k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_f1_60k)"
      ],
      "metadata": {
        "id": "sK5ItKvpucjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "th_di_60k = plot_experiment(\n",
        "    eval_type    = \"test\",\n",
        "    eval_dataset = \"rural\",\n",
        "    eval_years   = \"2018\",\n",
        "    threshold    = \"60000\",\n",
        "    metric       = \"disparate_impact\",\n",
        "    filters      = ([None, \"rural\", \"2018\"], True),\n",
        "    forced_filename = \"th_di_60k\",\n",
        "    save_png     = True)\n",
        "\n",
        "plt.show(th_di_60k)"
      ],
      "metadata": {
        "id": "tVzO2xi4ucjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Plotting\n",
        "\n",
        "Now that we have the main functionality working, we can start working on the dashboard to enable the user to easily access our collected metrics. To do that, we will define some IPython widgets."
      ],
      "metadata": {
        "id": "RYdtxRAqzTwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function which generates a suitable filename from the current dropdown menu parameters\n",
        "def get_filename_from_current_parameters():\n",
        "    return f\"{dropdown_eval_type.value}_{dropdown_eval_dataset.value}_{dropdown_eval_years.value}_{dropdown_threshold.value}_{dropdown_metric.value}\""
      ],
      "metadata": {
        "id": "Ghaa1-3JiWmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving format checkboxes\n",
        "checkbox_save_png = widgets.Checkbox(\n",
        "    description='PNG',\n",
        "    disabled=False,\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width=\"117px\")\n",
        ")      \n",
        "\n",
        "checkbox_save_pdf = widgets.Checkbox(\n",
        "    description='PDF',\n",
        "    disabled=False,\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width=\"117px\")\n",
        ")      "
      ],
      "metadata": {
        "id": "lrT1x3QD-RNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation type dropdown menu\n",
        "dropdown_eval_type = widgets.Dropdown(\n",
        "    options=['train', 'test'],\n",
        "    value='train',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "def on_change_dropdown_eval_type(change):\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "        dropdown_eval_dataset.options = update_dropdown_eval_dataset_by_filter(dropdown_eval_type.value)\n",
        "        dropdown_eval_years.options = update_dropdown_eval_years_by_filter(dropdown_eval_type.value, dropdown_eval_dataset.value)\n",
        "        dropdown_threshold.options = update_dropdown_threshold_by_filter(dropdown_eval_type.value, dropdown_eval_dataset.value, dropdown_eval_years.value)\n",
        "        vbox_saving.children[-2].children[0].value = get_filename_from_current_parameters()\n",
        "\n",
        "dropdown_eval_type.observe(on_change_dropdown_eval_type)"
      ],
      "metadata": {
        "id": "OtXy_DdK23dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation dataset dropdown menu\n",
        "\n",
        "def update_dropdown_eval_dataset_by_filter(filter_eval_type):\n",
        "    return list(dict.fromkeys(sorted(df_all[f\"dataset_{filter_eval_type}\"].to_list(), key=lambda x: dataset_name_to_pos(x, ignore_year=True))))\n",
        "\n",
        "initial_options_dropdown_eval_dataset = update_dropdown_eval_dataset_by_filter(dropdown_eval_type.value)\n",
        "\n",
        "dropdown_eval_dataset = widgets.Dropdown(\n",
        "    options=initial_options_dropdown_eval_dataset,\n",
        "    value=initial_options_dropdown_eval_dataset[0]\n",
        ")\n",
        "\n",
        "def on_change_dropdown_eval_dataset(change):\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "        dropdown_eval_years.options = update_dropdown_eval_years_by_filter(dropdown_eval_type.value, dropdown_eval_dataset.value)\n",
        "        dropdown_threshold.options = update_dropdown_threshold_by_filter(dropdown_eval_type.value, dropdown_eval_dataset.value, dropdown_eval_years.value)\n",
        "        vbox_saving.children[-2].children[0].value = get_filename_from_current_parameters()\n",
        "\n",
        "dropdown_eval_dataset.observe(on_change_dropdown_eval_dataset)"
      ],
      "metadata": {
        "id": "2c5ngTeS-Qow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation years dropdown menu\n",
        "\n",
        "def update_dropdown_eval_years_by_filter(filter_eval_type, filter_eval_dataset):\n",
        "    filtered_years = df_all.loc[(df_all[f\"dataset_{filter_eval_type}\"]==filter_eval_dataset)][f\"years_{filter_eval_type}\"].to_list()\n",
        "    return list(dict.fromkeys(sorted(filtered_years)))\n",
        "    \n",
        "initial_options_dropdown_eval_years = update_dropdown_eval_years_by_filter(dropdown_eval_type.value, dropdown_eval_dataset.value)\n",
        "\n",
        "dropdown_eval_years = widgets.Dropdown(\n",
        "    options=initial_options_dropdown_eval_years,\n",
        "    value=initial_options_dropdown_eval_years[0]\n",
        ")\n",
        "\n",
        "def on_change_dropdown_eval_years(change):\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "        dropdown_threshold.options = update_dropdown_threshold_by_filter(dropdown_eval_type.value, dropdown_eval_dataset.value, dropdown_eval_years.value)\n",
        "        vbox_saving.children[-2].children[0].value = get_filename_from_current_parameters()\n",
        "\n",
        "dropdown_eval_years.observe(on_change_dropdown_eval_years)"
      ],
      "metadata": {
        "id": "v9krEQdn_b_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# threshold dropdown menu\n",
        "\n",
        "def update_dropdown_threshold_by_filter(filter_eval_type, filter_eval_dataset, filter_eval_years):\n",
        "    filtered_thresholds = df_all.loc[(df_all[f\"dataset_{filter_eval_type}\"]==filter_eval_dataset) &\n",
        "                                     (df_all[f\"years_{filter_eval_type}\"]  ==filter_eval_years)][f\"threshold_{filter_eval_type}\"].to_list()\n",
        "    return list(dict.fromkeys(sorted(filtered_thresholds)))\n",
        "\n",
        "initial_options_dropdown_threshold = update_dropdown_threshold_by_filter(dropdown_eval_type.value, dropdown_eval_dataset.value, dropdown_eval_years.value)\n",
        "\n",
        "dropdown_threshold = widgets.Dropdown(\n",
        "    options=initial_options_dropdown_threshold,\n",
        "    value=initial_options_dropdown_threshold[0]\n",
        ")"
      ],
      "metadata": {
        "id": "8h-9rsCr_c2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metric dropdown menu\n",
        "dropdown_metric = widgets.Dropdown(\n",
        "    options=[\"abroca\", \"accuracy\", \"disparate_impact\", \"f1_score\"],\n",
        "    value=\"abroca\"\n",
        ")\n",
        "\n",
        "def on_change_dropdown_metric(change):\n",
        "    if change['type'] == 'change' and change['name'] == 'value':\n",
        "        vbox_saving.children[-2].children[0].value = get_filename_from_current_parameters()\n",
        "    elif change['type'] == 'change' and change['name'] == 'options':\n",
        "        if dropdown_metric.value not in [\"abroca\", \"accuracy\", \"disparate_impact\", \"f1_score\"]:\n",
        "            dropdown_metric.value = \"abroca\"\n",
        "        else:\n",
        "            dropdown_metric.value = val_old\n",
        "\n",
        "dropdown_metric.observe(on_change_dropdown_metric)"
      ],
      "metadata": {
        "id": "eFYTGgpy_e8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# legend location dropdown menu\n",
        "legend_locations = [\n",
        "    'best',\n",
        "    'upper left', 'upper center', 'upper right',\n",
        "    'center left', 'center', 'center right',\n",
        "    'lower left', 'lower center', 'lower right',\n",
        "    'none'\n",
        "]\n",
        "\n",
        "dropdown_legend_location = widgets.Dropdown(\n",
        "    options=legend_locations,\n",
        "    value=plt.rcParams[\"legend.loc\"]\n",
        ")"
      ],
      "metadata": {
        "id": "OR1KJxLM1tK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run button\n",
        "%matplotlib inline\n",
        "\n",
        "button_run = widgets.Button(\n",
        "    description=\"Display Plot\",\n",
        "    style={'description_width': 'initial',\n",
        "           'font_weight': 'bold'},\n",
        "    button_style='primary',\n",
        "    layout=Layout(width='435px', height='40px')\n",
        ")\n",
        "\n",
        "def on_click_button_run(b):\n",
        "    with output_plot:\n",
        "        ax = plot_experiment(\n",
        "            eval_type    = dropdown_eval_type.value,  \n",
        "            eval_dataset = dropdown_eval_dataset.value,\n",
        "            eval_years   = dropdown_eval_years.value,\n",
        "            threshold    = dropdown_threshold.value,\n",
        "            metric       = dropdown_metric.value,\n",
        "            save_png     = True,\n",
        "            save_pdf     = False,\n",
        "            forced_legend_loc = dropdown_legend_location.value\n",
        "        )\n",
        "\n",
        "        output_plot.clear_output()\n",
        "        plt.show(ax)\n",
        "    \n",
        "button_run.on_click(on_click_button_run)"
      ],
      "metadata": {
        "id": "Ipe8ihn4LWkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filename text field and saving button\n",
        "\n",
        "textfield_filename = Text(get_filename_from_current_parameters())\n",
        "\n",
        "button_save = widgets.Button(\n",
        "    description=\"Save\",\n",
        "    style={'description_width': 'initial'},\n",
        "    button_style='success',\n",
        "    layout=Layout(width='67px')\n",
        ")\n",
        "\n",
        "def on_click_button_save(b):\n",
        "    with output_stash:\n",
        "        ax = plot_experiment(\n",
        "            eval_type    = dropdown_eval_type.value,  \n",
        "            eval_dataset = dropdown_eval_dataset.value,\n",
        "            eval_years   = dropdown_eval_years.value,\n",
        "            threshold    = dropdown_threshold.value,\n",
        "            metric       = dropdown_metric.value,\n",
        "            save_png     = checkbox_save_png.value,\n",
        "            save_pdf     = checkbox_save_pdf.value,\n",
        "            forced_legend_loc = dropdown_legend_location.value,\n",
        "            forced_filename = textfield_filename.value\n",
        "        )\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "button_save.on_click(on_click_button_save)"
      ],
      "metadata": {
        "id": "hrQJSJUzJ1dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filename reset button\n",
        "button_reset = widgets.Button(\n",
        "    description=\"Reset\",\n",
        "    style={'description_width': 'initial'},\n",
        "    button_style='info',\n",
        "    layout=Layout(width='80px')\n",
        ")\n",
        "\n",
        "def on_click_button_reset(b):\n",
        "    vbox_saving.children[-2].children[0].value = get_filename_from_current_parameters()\n",
        "\n",
        "button_reset.on_click(on_click_button_reset)"
      ],
      "metadata": {
        "id": "xXKKtSo-jitR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# advanced metrics checkbox\n",
        "checkbox_advanced_metrics = widgets.Checkbox(\n",
        "    description='Show Advanced Metrics',\n",
        "    disabled=False,\n",
        "    style={'description_width': 'initial'},\n",
        "    layout=Layout(width=\"200px\")\n",
        ")\n",
        "\n",
        "val_old = None\n",
        "\n",
        "def on_trait_change_checkbox_advanced_metrics(b):\n",
        "    global val_old\n",
        "    val_old = dropdown_metric.value\n",
        "    simple_metrics = [\"abroca\", \"accuracy\", \"disparate_impact\", \"f1_score\"]\n",
        "    if checkbox_advanced_metrics.value:\n",
        "        dropdown_metric.options = df_all.columns[8:]\n",
        "    else:\n",
        "        if val_old not in simple_metrics: val_old = \"abroca\"\n",
        "        dropdown_metric.options = simple_metrics\n",
        "\n",
        "checkbox_advanced_metrics.observe(on_trait_change_checkbox_advanced_metrics)"
      ],
      "metadata": {
        "id": "Aui8EfPW3YK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# app\n",
        "%%capture\n",
        "output_plot = Output()\n",
        "output_stash = Output()\n",
        "\n",
        "header = HTML(\"<h1 style='font-family:verdana'>Interactive Fairness Monitoring</h1>\",\n",
        "              layout=Layout(height='auto',\n",
        "                            margin='0cm 0cm 0cm 5cm'))\n",
        "\n",
        "vbox_saving = VBox([HTML(\"<b style='font-size:18px'>Saving Name and Format</b>\"),\n",
        "                    HBox([textfield_filename,\n",
        "                          button_reset],\n",
        "                         layout=Layout(width=\"305px\")),\n",
        "                    HBox([checkbox_save_png,\n",
        "                          checkbox_save_pdf,\n",
        "                          button_save],\n",
        "                         layout=Layout(width=\"305px\"))                 \n",
        "                ])\n",
        "\n",
        "vbox_legend_pos = VBox([HTML(\"<b style='font-size:18px'>Legend Position</b>\"),\n",
        "                        dropdown_legend_location])\n",
        "\n",
        "app_layout = AppLayout(  \n",
        "    header=header,\n",
        "\n",
        "    left_sidebar=HBox([\n",
        "        HTML(\"<style>.left-one {margin-left: 1cm;}</style>\"),\n",
        "        VBox([\n",
        "            HTML(\"<b style='font-size:18px'>Parameters</b>\"),\n",
        "            Label(\"Evaluation Mode\"),\n",
        "            Label(\"Evaluation Dataset\"),\n",
        "            Label(\"Evaluation Year\"),\n",
        "            Label(\"Income Threshold\"),\n",
        "            Label(\"Metric\"),\n",
        "            Label()\n",
        "        ]).add_class(\"left-one\")\n",
        "    ]),\n",
        "    \n",
        "    center=VBox([\n",
        "        Label(),\n",
        "        dropdown_eval_type,\n",
        "        dropdown_eval_dataset,\n",
        "        dropdown_eval_years,\n",
        "        dropdown_threshold,\n",
        "        dropdown_metric,\n",
        "        checkbox_advanced_metrics\n",
        "    ]),\n",
        "    \n",
        "    right_sidebar=VBox([\n",
        "        vbox_legend_pos,\n",
        "        Label(),\n",
        "        vbox_saving,\n",
        "    ]),\n",
        "    \n",
        "    pane_widths=['160px', '350px', 1],\n",
        "    grid_gap=\"10px\"\n",
        ")\n",
        "\n",
        "button_run.add_class(\"top-three\")\n",
        "button_run.add_class(\"left-one\")\n",
        "output_plot.add_class(\"top-three\")\n",
        "output_plot.add_class(\"left-one\")"
      ],
      "metadata": {
        "id": "cpu57DQL2HkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# üîé **Interaction**\n",
        "\n",
        "When the setup finished, you can <font color='orange'>run the cell below</font> to start the interactive dashboard.\n",
        "\n",
        "Here, you can easily explore our data by creating your own plots. Simply select the parameters you are interested in and the corresponding plot will generate by itself, provided there are metrics for the selected parameters."
      ],
      "metadata": {
        "id": "4iLjNXUpz0xT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation Guidelines**\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Parameter</th>\n",
        "    <th>Meaning</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Evaluation Mode</td>\n",
        "    <td>Evaluate different models tested on the same dataset => mode <i>'test'</i> <br></br>Evaluate model trained on one dataset on multiple other datasets => mode <i>'train'</i></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Evaluation Dataset</td>\n",
        "    <td>For which dataset do you want to explore metrics?</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Evaluation Year</td>\n",
        "    <td>For which years do you want to explore metrics?</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Income Threshold</td>\n",
        "    <td>For which threshold do you want to explore metrics?</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Metric</td>\n",
        "    <td>Which metric do you want to plot?<br></br>Click <i>'Show Advanced Metrics'</i> to see all collected metrics.</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Legend Position</td>\n",
        "    <td>(Where) do you want the legend to be placed?<br></br>Choose <i>'best'</i> for automatic placement.</td>\n",
        "  </tr>  \n",
        "</table>\n",
        "<br/><br/>\n",
        "\n",
        "**Have fun exploring the metrics!** "
      ],
      "metadata": {
        "id": "e5Q277Ntr4xJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(app_layout)\n",
        "display(HTML(\"<style>.top-three {margin-top: .5cm;}</style>\"))\n",
        "display(HBox([button_run, HTML(\"<style>.left-onefive {margin-left: 1.45cm;}</style>\")]))\n",
        "display(output_plot)"
      ],
      "metadata": {
        "id": "Z_fhI4AA2MjA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}