# -*- coding: utf-8 -*-
"""Grid_training_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/136UZyBljW8siP663RSvay6WJwv6QB3l8

# üïπÔ∏è **Grid Training Notebook**

In this notebook, a training framework is developed which can train machine models on the ACIncome Task.




Furthermore, a dashboard is created which serves as an interface for the framework and allows the user to easily explore our collected metrics.

The purpose of this notebook is to allow students and researchers of AI fairness to explore how changes in temporal and/or spatial context can affect the fairness and performance of a model, as measured by various metrics. Recent research has shown that the interpretation of the fairness of classification models and datasets drastically depends on such context.

---
##‚öôÔ∏è Setup Grid Training V2

‚¨áÔ∏è <font color='orange'>Click on the arrow below to run the setup!</font>

### ‚öôÔ∏è Setup Data
"""

import numpy as np
import pandas as pd

#The Library that allows us to use the American Census Data and also offers the ACIncome Task
!pip install folktables

#Used for error correction of user input (state list)
!pip install python-Levenshtein

# needed for saving plotly plots
!pip install kaleido

"""**Define Dataset Helper Functions**

Label Data
"""

"""
function that labels numpy data (The output of the get_income_data_with_track function we defined) and returns a pandas Dataframe.
Most importantly it gives the 10 features their respective name.
"""

def label_numpy_data(data_array, labels=["AGEP","COW","SCHL","MAR","OCCP","POBP","RELP","WKHP","SEX","RAC1P"]):
  df = pd.DataFrame(data = data_array, columns = labels)
  return df


"""
Generate Pandas Dataframe from given inputs (of our get_income_data_with_track function). 
This will generate one huge dataframe that contains both the features and the label of the data.
This prepares the data for the conversion to AIF360 Binary Label Datasets.
"""

def Get_Dataframe_from_all(data_array, ground_truth, labels=["AGEP","COW","SCHL","MAR","OCCP","POBP","RELP","WKHP","SEX","RAC1P"]):
  df_label = pd.DataFrame(ground_truth,columns=['Label'])
  df_data = label_numpy_data(data_array,labels = labels)
  df = pd.concat([df_data,df_label],axis=1)
  return df

"""State Strings -> State Codes"""

from Levenshtein import distance as levenshtein_distance

"""function that calculates the nearest state given an input string list (e.g. "washintong" would (hopefully) be mapped to "Washington" """
def calc_nearest_state(input_list):

  all_states_to_compare_to = ["Alabama","Alaska","Arizona","Arkansas","California","Colorado","Connecticut","Delaware","District of Columbia","Florida","Georgia","Hawaii","Idaho","Illinois","Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland","Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana","Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York","North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania","Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah","Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming","Puerto Rico",]

  output_list = input_list.copy()

  for i in range(len(input_list)):
    cur_nearest_state = None
    cur_score = np.infty
    for state in all_states_to_compare_to:
      new_score = levenshtein_distance(input_list[i],state)
      if new_score<cur_score:
        cur_score = new_score
        cur_nearest_state = state

    output_list[i] = cur_nearest_state

  return output_list


"""
Example:
calc_nearest_state(["washingon","new macico"])
returns -> ['Washington', 'New Mexico']
"""

"""function that returns the state abbreviatian given the full name of a state."""


def get_state_code(states):
  state_code_list = []

  #define dict that given state name returns the state code
  state_name_to_code = {
      
      "Alabama" : "AL",
      "Alaska" : "AK",
      "Arizona" : "AZ",
      "Arkansas" : "AR",
      "California" : "CA",
      "Colorado" : "CO",
      "Connecticut" : "CT",
      "Delaware" : "DE",
      "District of Columbia" : "DC",
      "Florida" : "FL",
      "Georgia" : "GA",
      "Hawaii" : "HI",
      "Idaho" : "ID",
      "Illinois" : "IL",
      "Indiana" : "IN",
      "Iowa" : "IA",
      "Kansas" : "KS",
      "Kentucky" : "KY",
      "Louisiana" : "LA",
      "Maine" : "ME",
      "Maryland" : "MD",
      "Massachusetts" : "MA",
      "Michigan" : "MI",
      "Minnesota" : "MN",
      "Mississippi" : "MS",
      "Missouri" : "MO",
      "Montana" : "MT",
      "Nebraska" : "NE",
      "Nevada" : "NV",
      "New Hampshire" : "NH",
      "New Jersey" : "NJ",
      "New Mexico" : "NM",
      "New York" : "NY",
      "North Carolina" : "NC",
      "North Dakota" : "ND",
      "Ohio" : "OH",
      "Oklahoma" : "OK",
      "Oregon" : "OR",
      "Pennsylvania" : "PA",
      "Rhode Island" : "RI",
      "South Carolina" : "SC",
      "South Dakota" : "SD",
      "Tennessee" : "TN",
      "Texas" : "TX",
      "Utah" : "UT",
      "Vermont" : "VT",
      "Virginia" : "VA",
      "Washington" : "WA",
      "West Virginia" : "WV",
      "Wisconsin" : "WI",
      "Wyoming" : "WY",
      "Puerto Rico" : "PR",
  }
  for state in states:
    if len(str(state))==2:
      state_code_list.append(state)
    else:
      cur_state= calc_nearest_state([state])[0]
      state_code_list.append(state_name_to_code[cur_state])

  return state_code_list

#Example:

get_state_code(["Ilaska", "Arizona", "California", "Colorado", "Hawaii", "Idaho", "Montana", "Nevada", "Naw Maxico", "Oregon", "Utah", "WA", "Wyoming"])

"""Generate Data"""

"""function that gets the ACSIncome features, labels and group. While also returning a list of where each data point originated from"""

from folktables import ACSDataSource, ACSIncome, BasicProblem

def adult_filter(data):
    """Mimic the filters in place for Adult data.
    Adult documentation notes: Extraction was done by Barry Becker from
    the 1994 Census database. A set of reasonably clean records was extracted
    using the following conditions:
    ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))
    """
    df = data
    df = df[df['AGEP'] > 16]
    df = df[df['PINCP'] > 100]
    df = df[df['WKHP'] > 0]
    df = df[df['PWGTP'] >= 1]
    return df

"""
Our main function of generating data that we will use for our models.

Given a list of states and years it will download the relevant data and then return the wanted dataset(features),the labels, the group (Race) of each data sample and lists that contain the origin of the data (year, state).


"""

def get_income_data_with_track(states,years,threshold=50000,group_non_white=False,horizon='1-Year',survey='person'):
  """
  Inputs:
  states : list of strings (states)

  years : list of non-negative numbers in the range of 2014 - 2018 (years)

  threshold : non-negative number
  default value: 50000
  The threshold used for the ACIncome task. (Every data sample (income) above the threshold belongs to class 1 and every data sample (income) below the threshold belongs to class 0)

  group_non_white: Boolean
  default value: False
  Whether every non-white race should be relabeled as "Non-White" or not.
    If False: 9 Unique Race Values (1,2,3,4,5,6,7,8,9)
    If True:  2 Unique Race Values (-1,1)


  horizon='1-Year'
  Not relevant for a typical user, used as input for the folkstable library.

  survey='person'
  Not relevant for a typical user, used as input for the folkstable library.

  Returns:
  features : numpy array of arrays (the values of the features of each data sample, here 10 features)
  labels : numpy array of numbers (The class label of the data sample, see threshold)
  group : numpy array of numbers (The race of the data sample, see group_non_white)
  track_list_year : list of non-negative numbers in the range of 2014-2018 (years)
  track_list_state : list of strings (states)
  """
  for i in range(len(years)):
    years[i] = str(years[i])

  features = np.zeros((0,10))
  labels = np.zeros((0))
  group = np.zeros((0))
  track_list_year = []
  track_list_state = []

  states = get_state_code(states)

  IncomeProblem_own = BasicProblem(
      features=[
          'AGEP',
          'COW',
          'SCHL',
          'MAR',
          'OCCP',
          'POBP',
          'RELP',
          'WKHP',
          'SEX',
          'RAC1P',
      ],
      target='PINCP',
      target_transform=lambda x: x > threshold,
      group='RAC1P',
      preprocess=adult_filter,
      postprocess=lambda x: np.nan_to_num(x, -1),
    )

  for year in years:
    for state in states:
      data_source = ACSDataSource(survey_year=year, horizon=horizon, survey=survey)
      orig_data = data_source.get_data(states=[state], download=True)
      orig_features, orig_labels, orig_group = IncomeProblem_own.df_to_numpy(orig_data)

      if group_non_white==True:
        #1 stays 1 and everything else becomes -1
        orig_features[:,9] = 2*(orig_features[:,9]==1) -1
        orig_group = 2*orig_group[orig_group==1] -1

      features = np.concatenate((features, orig_features))
      labels = np.concatenate((labels, orig_labels))
      group = np.concatenate((group, orig_group))

      for i in range(len(orig_group)):
        track_list_year.append(int(year))
        track_list_state.append(state)


  return features,labels,group,track_list_year,track_list_state

"""### ‚öôÔ∏è Setup Model"""

!pip install aif360

!pip install -q fairlearn

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# Load all necessary packages
import sys
sys.path.append("../")
from aif360.datasets import BinaryLabelDataset
#from aif360.datasets import AdultDataset, GermanDataset, CompasDataset
# from aif360.metrics import BinaryLabelDatasetMetric
# from aif360.metrics import ClassificationMetric
# from aif360.metrics.utils import compute_boolean_conditioning_vector
#from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult, load_preproc_data_compas, load_preproc_data_german
#from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing
#from aif360.algorithms.inprocessing.meta_fair_classifier import MetaFairClassifier
#from sklearn.linear_model import LogisticRegression
#from sklearn.preprocessing import StandardScaler, MaxAbsScaler
#from sklearn.metrics import accuracy_score

from IPython.display import Markdown, display
import matplotlib.pyplot as plt

import tensorflow as tf
tf.compat.v1.disable_eager_execution()

"""####  aif360  inprocessing models

Our focus to handle bias lies on **"In-processing"** algorithms which we took from the aif360 library.  
AI Fairness 360 (aif360) is a open source library containing techniques to detect and mitigate bias in datasets and models.  
In-processing algorithms are bias mitigation algorithms which are applied to a model during its training.  
We took used thre In-Processing techniques from the aif360 library whiche are AversialDebiasing, MetafairClassifier and Prejudice Remover.
For further information about the In-processing algorithms we linked the papers at the "References" paragraph of their description.

##### AdversialDebiasing

<font color='#3687bd'>class</font> **aif360.algorithms.inprocessing.AdversarialDebiasing**<font color='#3687bd'>(unprivileged_groups, privileged_groups, scope_name, sess, seed=None, adversary_loss_weight=0.1, num_epochs=50, batch_size=128, classifier_num_hidden_units=200, debias=True)</font>

Adversarial debiasing is an in-processing technique that learns a classifier to maximize prediction accuracy and simultaneously reduce an adversary‚Äôs ability to determine the protected attribute from the predictions [5]. This approach leads to a fair classifier as the predictions cannot carry any group discrimination information that the adversary can exploit.  

**Parameters:**
*   **unprivileged_groups** (<font color='#ab74c1'>tuple</font>) ‚Äì Representation for unprivileged groups
*   **privileged_groups** (<font color='#ab74c1'>tuple</font>) ‚Äì Representation for privileged groups  
*   **scope_name** (<font color='#ab74c1'>str</font>) ‚Äì scope name for the tenforflow variable
*   **sess** (tf.Session) ‚Äì tensorflow session
*   **seed** (<font color='#3687bd'>int</font>, optional) ‚Äì Seed to make **predict** repeatable
*   **adversary_loss_weight** ((<font color='#3687bd'>float</font>, optional) ‚Äì Hyperparameter that chooses the strength of the adversarial loss
*   **num_epochs** ((<font color='#3687bd'>int</font>, optional) ‚Äì Number of training epochs
*   **batch_size** ((<font color='#3687bd'>int</font>, optional) ‚Äì Batch size
*   **classifier_num_hidden_units** ((<font color='#3687bd'>int</font>, optional) ‚Äì Number of hidden units in the classifier model
*   **debias** ((<font color='#3687bd'>bool</font>, optional) ‚Äì Learn a classifier with or without debiasing  

**References**  
[5]	B. H. Zhang, B. Lemoine, and M. Mitchell, ‚ÄúMitigating Unwanted Biases with Adversarial Learning,‚Äù AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society, 2018.

**aif360 doc:**  
https://aif360.readthedocs.io/en/stable/modules/generated/aif360.algorithms.inprocessing.AdversarialDebiasing.html#id2
"""

from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing

"""**Experience**  
The AdversialDebiasing algorithm has the option to learn a classifier with or without debiasing by setting the debias paramete to true or false which we used to compare the results.
We 
The run time of the model w
(Meta fair-> ~1,5 sec pro 10k
Adversial Debiasing -> ~15,1 Sec pro 10k
Prejudice Remover-> ~16,6 sec pro 10k 
- data set
- speed 
- besonderheite

##### MetafairClassifier

<font color='#3687bd'>class</font> **aif360.algorithms.inprocessing.MetaFairClassifier**<font color='#3687bd'>(tau=0.8, sensitive_attr='', type='fdr', seed=None)</font>

The meta algorithm here takes the fairness metric as part of the input and returns a classifier optimized w.r.t. that fairness metric [11]. 

**Parameters:**
*   **tau** (double, optional) ‚Äì Fairness penalty parameter
*   **sensitive_attr** (<font color='#ab74c1'>str</font>, optional) ‚Äì Name of protected attribute.  
*   **type** (<font color='#ab74c1'>str</font>, optional) ‚Äì The type of fairness metric to be used. Currently ‚Äúfdr‚Äù (false discovery rate ratio) and ‚Äúsr‚Äù (statistical rate/disparate impact) are supported. To use another type, the corresponding optimization class has to be implemented.
*   **seed** (int, optional) ‚Äì Random seed.

**References**  
[11]	L. E. Celis, L. Huang, V. Keswani, and N. K. Vishnoi. ‚ÄúClassification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees,‚Äù 2018.


**aif360 doc:**  
https://aif360.readthedocs.io/en/stable/modules/generated/aif360.algorithms.inprocessing.MetaFairClassifier.html
"""

from aif360.algorithms.inprocessing.meta_fair_classifier import MetaFairClassifier

"""- ist nur auf

##### PrejudiceRemover

<font color='#3687bd'>class</font> **aif360.algorithms.inprocessing.PrejudiceRemover**<font color='#3687bd'>(eta=1.0, sensitive_attr='', class_attr='')</font>

Prejudice remover is an in-processing technique that adds a discrimination-aware regularization term to the learning objective [6].  

**Parameters:**
*   **eta** (double, optional) ‚Äì fairness penalty parameter
*   **sensitive_attr** (<font color='#ab74c1'>str</font>, optional) ‚Äì name of protected attribute  
*   **class_attr** (<font color='#ab74c1'>str</font>, optional) ‚Äì label name

**References**  
[6]	T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma, ‚ÄúFairness-Aware Classifier with Prejudice Remover Regularizer,‚Äù Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 2012.


**aif360 doc:**  
https://aif360.readthedocs.io/en/stable/modules/generated/aif360.algorithms.inprocessing.PrejudiceRemover.html
"""

from aif360.algorithms.inprocessing.prejudice_remover import PrejudiceRemover

"""### ‚öôÔ∏è Setup Pickle

Pickle is a library that allows us to generate files out of complex variables.

We use it to save our train datasets and the predictions of the model.
"""

import pickle

"""### ‚öôÔ∏è Setup Metrics

In this section, we setup the metrics that will be evaluated for each model. 
<br></br>
For that, we use all the metrics of `aif360.metrics.ClassificationMetric` ([docs](https://aif360.readthedocs.io/en/stable/modules/generated/aif360.metrics.ClassificationMetric.html)), which is a mix of performance and fairness related metrics. Additionally, we compute the ABROCA (actually just BROCA, since we do not take the absolute value to retain information about the favored group). Finally, we store all metrics in a csv-file, which will be used in our `metrics-visualization.ipynb` notebook.

ABROCA is a powerful fairness metric used in predictive modeling [proposed in 2019 by Gardner et. al](https://dl.acm.org/doi/10.1145/3303772.3303791), which is one of the main metrics we will use to assess model fairness throughout this project. It is formally defined as:

![Screenshot 2022-08-08 204927.png](data:image/png;base64,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)

Another important metric for assessing fairness is Disparate Impact, which is defined in the `aif360` library. The formula for disparate impact is the following:

![Screenshot 2022-08-08 204821.png](data:image/png;base64,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)

Still, it is important that we collect all the other metrics, too, as we want to allow the users to explore the collected metrics themselves.
"""

import pandas as pd
from sklearn.metrics import roc_auc_score

# function to compute ABROCA given two datasets
def abroca(test_dataset, pred_dataset):
    df_test, _ = test_dataset.convert_to_dataframe()
    df_pred, _ = pred_dataset.convert_to_dataframe()

    df = pd.concat([df_test["Label"], df_pred["Label"], df_test["RAC1P"]],
                   axis=1,
                   keys=["gt", "pred", "race"])

    return compute_abroca(df)

# function to compute ABROCA given a dataframe with columns race, gt and pred
def compute_abroca(df):
    roc_auc_WHITE = roc_auc_score(y_true=df[df["race"] == 1]["gt"],
                              y_score=df[df["race"] == 1]["pred"])

    roc_auc_OTHER = roc_auc_score(y_true=df[df["race"] != 1]["gt"],
                              y_score=df[df["race"] != 1]["pred"])

    return roc_auc_WHITE - roc_auc_OTHER

from aif360.datasets import BinaryLabelDataset
from aif360.metrics import ClassificationMetric

import pickle
import csv

METRIC_NAMES = ['model_name',
                'abroca',
                'accuracy',
                'average_abs_odds_difference',
                'average_odds_difference',
                'base_rate',
                'between_all_groups_coefficient_of_variation',
                'between_all_groups_generalized_entropy_index',
                'between_all_groups_theil_index',
                'between_group_coefficient_of_variation',
                'between_group_generalized_entropy_index',
                'between_group_theil_index',
                'binary_confusion_matrix',
                'coefficient_of_variation',
                'consistency',
                'difference',
                'differential_fairness_bias_amplification',
                'disparate_impact',
                'equal_opportunity_difference',
                'error_rate',
                'error_rate_difference',
                'error_rate_ratio',
                'false_discovery_rate',
                'false_discovery_rate_difference',
                'false_discovery_rate_ratio',
                'false_negative_rate',
                'false_negative_rate_difference',
                'false_negative_rate_ratio',
                'false_omission_rate',
                'false_omission_rate_difference',
                'false_omission_rate_ratio',
                'false_positive_rate',
                'false_positive_rate_difference',
                'false_positive_rate_ratio',
                'generalized_binary_confusion_matrix',
                'generalized_entropy_index',
                'generalized_false_negative_rate',
                'generalized_false_positive_rate',
                'generalized_true_negative_rate',
                'generalized_true_positive_rate',
                'mean_difference',
                'negative_predictive_value',
                'num_false_negatives',
                'num_false_positives',
                'num_generalized_false_negatives',
                'num_generalized_false_positives',
                'num_generalized_true_negatives',
                'num_generalized_true_positives',
                'num_instances',
                'num_negatives',
                'num_positives',
                'num_pred_negatives',
                'num_pred_positives',
                'num_true_negatives',
                'num_true_positives',
                'performance_measures',
                'positive_predictive_value',
                'power',
                'precision',
                'ratio',
                'recall',
                'rich_subgroup',
                'selection_rate',
                'sensitivity',
                'smoothed_empirical_differential_fairness',
                'specificity',
                'statistical_parity_difference',
                'theil_index',
                'true_negative_rate',
                'true_positive_rate',
                'true_positive_rate_difference']

def compute_metrics(test_dataset,
                    prediction_dataset,
                    unprivileged_groups=[{'RAC1P': 2,'RAC1P': 3,'RAC1P': 4,'RAC1P': 5,'RAC1P': 6,'RAC1P': 7,'RAC1P': 8,'RAC1P': 9}],
                    privileged_groups=[{'RAC1P': 1}],
                    write_csv=True,
                    csv_name="metrics.csv",
                    model_name="<no model name>"):
    """
    Given BinaryLabelDatasets test_dataset and prediction_dataset, compute all
    AIF360 ClassificationMetrics and optionally store them in a .csv file.

    Additionally, return a dictionary of all metrics.
    """

    # create ClassificationMetric object with test set and predictions
    # the object will include all metrics, see link below
    # https://aif360.readthedocs.io/en/stable/modules/generated/aif360.metrics.ClassificationMetric.html

    classification_metrics = ClassificationMetric(test_dataset,
                                                  prediction_dataset,
                                                  unprivileged_groups,
                                                  privileged_groups)

    metric_values = []

    # compute all metrics specified in METRIC_NAMES
    for metric_name in METRIC_NAMES[2:]:
        try:
            loc = {"classification_metrics": classification_metrics}
            exec(f"value = classification_metrics.{metric_name}()", globals(), loc)
            metric_values.append(loc["value"])
        except TypeError:
            metric_values.append(None)

    # insert ABROCA
    metric_values.insert(0, abroca(test_dataset, prediction_dataset))

    # insert model name
    metric_values.insert(0, model_name)

    # dictionary of all metrics in name:value format
    metrics_dict = dict(zip(METRIC_NAMES, metric_values))

    # write metrics to .csv file, if enabled
    if write_csv:
        with open(csv_name, 'w', encoding='utf8') as f:
            writer = csv.writer(f)
            writer.writerow(METRIC_NAMES)
            writer.writerow(metric_values)

    return metrics_dict

"""### ‚öôÔ∏è Setup US Region Plotting"""

import plotly.express as px
import pandas as pd
import numpy as np
import random
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from Levenshtein import distance as levenshtein_distance

def plot_us_states(state_codes, file_name=None):
    """
    Generate a plot of given US states.

    Args:
        state_codes (list of str): State codes to plot.
        file_name (str, optional): File name for generated plot. Defaults to None,
                                   which means that the plot does not get saved.
    """
    
    states_values = [[state, random.rand()] for state in state_codes]
    df = pd.DataFrame(states_values, columns=['state_code', 'value'])
    fig = px.choropleth(df,
                        locations='state_code',
                        locationmode='USA-states',
                        scope='usa',
                        color='value')

    fig.update_coloraxes(showscale=True)

    if file_name is not None:
        fig.write_image(file_name)

    fig.show()

def plot_us_regions_and_states(regions, metric_values, metric=None, infos=None, file_name_pre=None):
    """
    Generate a plot of all regions together and a single plot for every region.

    Args:
        regions (list of str): region codes to plot.
        metric_values (list of int): the values for the metric
        metric (str): name of the used metric changes the used color range
          possibilitys: "accuracy", "abroca", "f1score", "disparate impact"
          if none -> "Viridis" will be used as colorscale and the range will be
          the min and max value from metric_vales
        infos (dict): a dictionary with information about the plots
          example dict {trained_dataset = 'west coast geo',
                        year = 2015,
                        threshold = 500000,
                        test_dataset = ['rural','west coast geo'],
                        model_name = 'Metafair'}
        file_name_pre (str, optional): adds a prefix to the generated filenames 
    """
    plot_us_states_cholormap(regions, metric_values, metric=metric, infos=infos, file_name_pre=file_name_pre)
    plot_us_regions2(regions, metric_values, metric=metric, infos=infos, file_name_pre=file_name_pre)

def plot_us_regions2(regions, metric_values, metric=None, infos=None, file_name_pre=None):
    """
    Generate a single plot for every given US region.

    Args:
        regions (list of str): region codes to plot.
        metric_values (list of int): the values for the metric
        metric (str): name of the used metric changes the used color range
          possibilitys: "accuracy", "abroca", "f1score", "disparate impact"
          if none -> "Viridis" will be used as colorscale and the range will be
          the min and max value from metric_vales
        infos (dict): a dictionary with information about the plots
          example dict {trained_dataset = 'west coast geo',
                        year = 2015,
                        threshold = 500000,
                        test_dataset = ['rural','west coast geo'],
                        model_name = 'Metafair'}
        file_name_pre (str, optional): adds a prefix to the generated filenames
    """
    states = regions_to_states(regions)
    state_code_regions = []
    for state_region in states:
      state_code_regions.append(get_state_code(state_region))
    color_continuous_midpoint = None
    continous_scale = "Viridis"
    metric_name = metric
    if not metric:
      range_color = [min(metric_values),max(metric_values)]
      metric_name = "value"
    elif metric == "accuracy":
      range_color = [0,1]
      continous_scale = "RdYlGn"
    elif metric == "abroca":
      range_color = [-1,1]
      continous_scale = "PuOr"
      color_continuous_midpoint=0
    elif metric == "f1score":
      continous_scale = "RdYlGn"
      range_color = [0,1]
    elif metric == "disparate impact":
      continous_scale = "PuOr"
      range_color = [0,2]
      color_continuous_midpoint=1
    else:
      range_color = [min(metric_values),max(metric_values)]
    figures= []
    for count, state_code_region in enumerate(state_code_regions):
      states_values = []
      for state_code in state_code_region:
        states_values.append([state_code, metric_values[count],regions[count]])
      df = pd.DataFrame(states_values, columns=['state_code', metric_name,'regions'])
      if not infos:
        title = regions[count]
      else:
        title=""
        title = title + "Trained " + infos["model_name"]+ " "+ "on date from"+ " "+ infos["year"]+ " "+ "with a threshold of" + " " +infos["threshold"]+"<br>"
        title = title + "Train Dataset: " + infos["trained_dataset"]+ "<br>"
        title = title + "Test Dataset: " + regions[count] + " "
      fig = px.choropleth(df,
                          locations='state_code',
                          locationmode='USA-states',
                          scope='usa',
                          color= metric_name,
                          range_color= range_color,
                          color_continuous_scale=continous_scale,
                          color_continuous_midpoint=color_continuous_midpoint,
                          hover_data=[metric_name, 'regions'],
                          title=title)
      fig.update_coloraxes(showscale=True)
      figures.append(fig)
    for count, f in enumerate(figures):
      file_name= ""
      if file_name_pre is not None:
        file_name = file_name_pre+ "_"
      if infos is not None:
        file_name = file_name + infos["model_name"]+ "_"+  infos["trained_dataset"]+ "_"+ infos["year"]+ "_"+infos["threshold"]+"_"+ regions[count] + "_"+ infos["year"]+ "_"+infos["threshold"]
      else:
        file_name = file_name + regions[count]
      file_name = file_name + ".png"
      f.write_image(file_name)
      f.show()

def plot_us_states_cholormap(regions, metric_values, metric=None, infos=None, file_name_pre=None, ):
    """
    Generate a plot of given US regions.

    Args:
        regions (list of str): region codes to plot.
        metric_values (list of int): the values for the metric
        metric (str): name of the used metric changes the used color range
          possibilitys: "accuracy", "abroca", "f1score", "disparate impact"
          if none -> "Viridis" will be used as colorscale and the range will be
          the min and max value from metric_vales
        infos (dict): a dictionary with information about the plots
          example dict {trained_dataset = 'west coast geo',
                        year = 2015,
                        threshold = 500000,
                        test_dataset = ['rural','west coast geo'],
                        model_name = 'Metafair'}
        file_name_pre (str, optional): adds a prefix to the generated filenames
    """
    states = regions_to_states(regions)
    state_code_regions = []

    for state_region in states:
      state_code_regions.append(get_state_code(state_region))
    color_continuous_midpoint=None
    continous_scale = "Viridis"
    metric_name = metric
    if not metric:
      range_color = [min(metric_values),max(metric_values)]
      metric_name = "value"
    elif metric == "accuracy":
      range_color = [0,1]
      continous_scale = "RdYlGn"
    elif metric == "abroca":
      range_color = [-1,1]
      continous_scale = "PuOr"
      color_continuous_midpoint=0
    elif metric == "f1score":
      continous_scale = "RdYlGn"
      range_color = [0,1]
    elif metric == "disparate impact":
      continous_scale = "PuOr"
      range_color = [0,2]
      color_continuous_midpoint=1
    else:
      range_color = [min(metric_values),max(metric_values)]
    if not infos:
      title = "all selected data sets (mean values)"
    else:
      title=""
      title = title + "Trained " + infos["model_name"]+ " "+ "on date from"+ " "+ infos["year"]+ " "+ "with a threshold of" + " " +infos["threshold"]+"<br>"
      title = title + "Train Dataset: " + infos["trained_dataset"]+ "<br>"
      title = title + "Test Dataset: " + ", ".join(regions)+ " "
    states_values = []
    state_code_dict = {}
    for count, state_code_region in enumerate(state_code_regions):
      for state_code in state_code_region:
        if state_code in state_code_dict.keys():
          state_code_dict[state_code].append((metric_values[count],regions[count]))
        else:
          state_code_dict[state_code]= [(metric_values[count],regions[count])]
    for key_state_code in state_code_dict.items():
      state_code_regions= []
      values=[]
      for value_region_tuple in key_state_code[1]:
        values.append(value_region_tuple[0])
        state_code_regions.append(value_region_tuple[1])
      mean = sum(values)/len(key_state_code[1])
      states_values.append([key_state_code[0], mean, state_code_regions, values])
    df = pd.DataFrame(states_values, columns=['state_code', metric_name,'regions', 'values'])
    fig = px.choropleth(df,
                        locations='state_code',
                        locationmode='USA-states',
                        scope='usa',
                        color=metric_name,
                        color_continuous_scale=continous_scale,
                        color_continuous_midpoint=color_continuous_midpoint,
                        range_color= range_color,
                        hover_data=[metric_name, 'regions', 'values'],
                        title=title
                        )
    fig.update_coloraxes(showscale=True)
    file_name = ""
    if file_name_pre is not None:
      file_name = file_name_pre + "_"
    regions_name = "_".join(regions)
    if infos is not None:
      file_name = file_name + infos["model_name"]+ "_"+  infos["trained_dataset"]+ "_"+ infos["year"]+ "_"+infos["threshold"]+"_"+ regions_name + "_"+ infos["year"]+ "_"+infos["threshold"]
    else:
      file_name = file_name +  regions_name
    file_name = file_name + ".png"
    fig.write_image(file_name)
    fig.show()

def regions_to_states(regions: list):
  """ Gets regions and returns the euqivalent states for the regions

  Args:
      regions (list): gets a list of regions

  Returns:
      list: gives a list of list where every list stand for a region with his states

  Example:
      regions_to_states(['west coast geo', 'urban']) ->  [['Alaska', 'California', 'Hawaii', 'Oregon', 'Washington'],['California ','New Jersey ',...]]
  """
  states = []
  for cur_dataset in regions:
      #Determine current state list according to dataset preset
      if cur_dataset == 'west coast geo':
        states.append(["Alaska", "California", "Hawaii", "Oregon", "Washington"])
      elif cur_dataset == 'west coast wiki':
        states.append(["Alaska", "Arizona", "California", "Colorado", "Hawaii", "Idaho", "Montana", "Nevada", "New Maxiko", "Oregon", "Utah", "Washington", "Wyoming"])
      elif cur_dataset == 'east coast geo':
        states.append(["Maine", "New Hampshire", "Massachusetts", "Rhode Island", "Connecticut", "New York", "New Jersey", "Delaware", "Maryland", "Virginia", "North Carolina", "South Carolina", "Georgia", "Florida"])
      elif cur_dataset == 'none coast geo':
        states.append(["Alabama", "Arkansas", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", "Michigan", "Minnesota", "Mississippi", "Missouri", "Nebraska", "North Dakota", "Ohio", "Oklahoma", "Pennsylvania", "South Dakota", "Tennessee", "Texas", "Vermont", "West Virginia", "Wisconsin", "Puerto Rico", "Colorado", "Wyoming", "Montana", "Idaho", "Utah", "Nevada", "Arizona", "New Mexiko"])
      elif cur_dataset == 'none coast wiki':
        states.append(["Alabama", "Arkansas", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", "Michigan", "Minnesota", "Mississippi", "Missouri", "Nebraska", "North Dakota", "Ohio", "Oklahoma", "Pennsylvania", "South Dakota", "Tennessee", "Texas", "Vermont", "West Virginia", "Wisconsin", "Puerto Rico", "Colorado", "Wyoming", "Montana", "Idaho", "Utah", "Nevada", "Arizona", "New Mexiko"])
      elif cur_dataset == 'urban':
        states.append(["California ","New Jersey ","Nevada ","Puerto Rico ","Massachusetts ","Hawaii ","Florida","Rhode Island ","Utah ","Arizona ","Illinois ","Connecticut ","New York ","Maryland ","Colorado ","Texas ","Washington ","Delaware ","Oregon "])
      elif cur_dataset == 'rural':
        states.append(["Pennsylvania", "Ohio", "New Mexico", "Virginia", "Georgia", "Michigan", "Kansas", "Minnesota", "Louisiana", "Nebraska", "Indiana", "Idaho", "Missouri", "Wisconsin", "Tennessee", "South Carolina", "Oklahoma", "North Carolina", "Alaska", "Wyoming", "Iowa", "New Hampshire", "North Dakota", "Alabama", "Kentucky", "South Dakota", "Arkansas", "Montana", "Mississippi", "West Virginia", "Vermont", "Maine", ])
      elif cur_dataset == 'north':
        states.append(["Washington","Oregon","Idaho","Montana","Wyoming","North Dakota","South Dakota","Nebraska","Minnesota","Iowa","Wisconsin","Illinois","Michigan","Indiana","Ohio","Pennslyvania","New York","Vermont","New Hampshire","Maine","Massasuchets","Conniticet","New Jersey"])
      elif cur_dataset == 'south':
        states.append(["California","Nevada","Utah","Arizona","New Mexico","Colorado","Texas","Oklahoma","Kansas","Arkansas","Missouri","Louisana","Missisipi","Alabama","Tenesee","Kentucky","Georgia","Florida","South Carolina","North Carolina","Virginia","West Virginia","Delaware","Maryland"])
      elif cur_dataset == 'all':
        states.append(["California","Nevada","Utah","Arizona","New Mexico","Colorado","Texas","Oklahoma","Kansas","Arkansas","Missouri","Louisana","Missisipi","Alabama","Tenesee","Kentucky","Georgia","Florida","South Carolina","North Carolina","Virginia","West Virginia","Delaware","Maryland","Washington","Oregon","Idaho","Montana","Wyoming","North Dakota","South Dakota","Nebraska","Minnesota","Iowa","Wisconsin","Illinois","Michigan","Indiana","Ohio","Pennslyvania","New York","Vermont","New Hampshire","Maine","Massasuchets","Conniticet","New Jersey"])
      else: 
        states.append([])
  return states

"""### ‚öôÔ∏è Setup Helper functions

Imports and helper functions that are not directly related to machine learning etc.
"""

import sys, os

"""
deafen can be used to reduce the output a function call generates.
This is especially useful for the AIF360 models, as they don't have an inherent verbose parameter.
"""

def deafen(function, *args):
    real_stdout = sys.stdout
    sys.stdout = open(os.devnull, "w")
    output = function(*args)
    sys.stdout = real_stdout
    return output

##Credit Kamron Bhavnagri from Stackoverflow

import sys
import random

"""### ‚öôÔ∏è Setup Grid Training

**The main function of our training framework.**

It combines every previously introduced method into one singular call. 

Given all of the parameters, the method will train the selected model(s) on the selected data with all of the relevant parameters set as defined.

How?
Summary:

      *   Check inputs
      *   Download and generate all selected datasets
      *   Split data into train and test sets (Holdout Testing)
      *   Save every train dataset
      Repeat for every model:
        Repeat for every year:
          Repeat for every threshold:
            Repeat for every train dataset:
              *   Train model on the dataset
              *   Predict data for every dataset using trained model
              *   Evaluate the predictions -> generate metrics
              *   Save predictions and metrics
      * If verbose is high enough: output plot of selected metric
        

Why in this way?

This ensures that all datasets are evaluated fairly. 

It also allows uhe user to the directly if the results are dependent on the direction 
(e.g. maybe model from 2014 works better than a model from 2018 on data from 2018, but is this also the case for the data from 2014? We can check this easily)

It also allows the user to be really flexible in what they want, without an extensive list of inputs. 
E.g. if the user had to add every test dataset individually it would reduce readibility and discourages users from experimenting as the syntax would be too complex.

Each execution of this function generates a lot of data, however depending on what the user is interested in many things can be overlooked (Still the possibility of using it is already provided)
"""

"""
EXAMPLE PARAMETERS
datasets = ['west coast geo','east coast geo', 'none coast geo', 'urban','rural','north','south','all']

years = ['2014','2015','2016','2017','2018','2014 2015 2016']

models = ['adversial debiasing']

thresholds = ['50000']

group_non_white = False

verbose = 1

seed = 12681

metric = 'accuracy'

file_prefix = None

"""


def grid_train_model_v2(datasets : list,years : list, models : list,thresholds = ['50000'],group_non_white = False,verbose=1,seed = 12681, metric='accuracy',file_prefix=None):
  """
  Given a list of datasets, a list of years, a list of models, and optionally a list of thresholds, a list of additional paramaters and a boolean for grouping none whites,
    the function trains all of the models on all of the datasets in accordance with the years, threshold and the none white grouping.

  Args:
      datasets (list of strings): List of datasets that should be evaluated. Each Model is trained on one of the datasets and evaluated on all datasets. Supported presets = 'west coast geo','west coast wiki','east coast geo','none coast geo','none coast wiki','urban','rural','north','south','all'
      years (list of strings): List of years of the datasets. Possible values are 2014-2018 (inclusive) and also a combination of different years separated by a space. e.g. '2014' or '2014 2015 2016'
      models (list of strings): List of models that should be evaluated. Each Model is trained both fairness aware and unaware. Supported models = 'adversial debiasing'
      threshholds (list of strings, optional): List of thresholds that should be used to determine the labels of the datasets. By default it is ['50000'], the split used by the 'Adult Dataset' 
      parameters (list, optional): list of parameters that should be given to the models. Currently not used.
      group_non_white(Bool,optional): Boolean Value that decides if non-white heritages are either grouped together or not. RAC1P = [-1,1] if True, RAC1P = [1,2,3,4,5,6,7,8,9] if False

  Outputs:
      Preds_'model'_'debias'_'trained dataset'_'years'_'threshold'_'tested dataset'_'years'_'threshold'_'group non-white'(pickled BinaryLabelDatasets): Both the predictions 

  Returns:
      (list): predctions
      (list): labels
      (list): groups

  ----------------------------------------------------------------
      datasets
      Possible Values: ['west coast geo','east coast geo', 'none coast geo', 'urban','rural','north','south','all']

      Subgroup of our supported datasets. Every dataset as a single string.
      years 
      Possible Values: ['2014','2015','2016','2017','2018','2014 2015 2016']

      Every Year from 2014 - 2018 (inclusive) or a subgroup of those years. Everything as a single string

      models 
      Possible Values: ['adversial debiasing', 'Metafair','PrejudiceRemover']

      Subgroup of our supported Models. Every model as a single string.

      Threshold 
      Possible Values: Any nonnegative number. Number should be a string. 
      Reasonable numbers could be: 30000,40000,50000 etc.

      group_non_white
      Possible Values: [True, False]

      Whether every non-white race should be relabeled as "Non-White" or not.
      If False: 9 Unique Race Values
      If True:  2 Unique Race Values

      verbose
      Possible Values: [0,1,2]

      Determines how much output the User receives during the training. 
      0 = no outputs, 1 = medium amount of output, 2 = high amount of output

      seed
      Possible Values: Any Number

      Used as the initalization for the random number generation (e.g. affects the train-test split of the data)

      metric
      Possible Values: ['accuracy','f1score','abroca','disparate impact']

      Determines the metric used for an overview plot at the end. The plot shows the value of the metric for the last trained model on every dataset (in the last year + threshold).

      file_prefix
      Possible Values: Any String

      The prefix for the filename of the generated plots
      """

  #Assert Datasets are as intended
  assert datasets is not None
  supported_sets = ['west coast geo','west coast wiki','east coast geo','none coast geo','none coast wiki','urban','rural','north','south','all']
  for cur_dataset in datasets:
    assert cur_dataset in supported_sets 

  #Assert years are as intended
  assert years is not None
  supported_years = ['2014','2015','2016','2017','2018']
  for cur_years in years:
    splitted_years = cur_years.split()
    for year in splitted_years:
      assert year in supported_years

  #Assert models are as intended
  assert models is not None
  supported_models = ['adversial debiasing', 'Metafair','PrejudiceRemover']
  for cur_model in models:
    assert cur_model in supported_models


  #Assert thresholds can be transformed into integers
  assert thresholds is not None
  for i in range(len(thresholds)):
    threshold = thresholds[i]
    try:
      thresholds[i] = int(threshold)
    except TypeError as err:
      print('Error in Transformation:',err)


  print('Now starting the Grid Training')

  """
  Dataset Section
  """

  list_of_all_datasets_train = []
  list_of_all_datasets_test = []
  name_of_datasets = []
  list_of_years = []
  list_of_thresholds = []

  for cur_dataset in datasets:
    #Determine current state list according to dataset preset
    if cur_dataset == 'west coast geo':
      state_list = ["Alaska", "California", "Hawaii", "Oregon", "Washington"] 
    elif cur_dataset == 'west coast wiki':
      state_list = ["Alaska", "Arizona", "California", "Colorado", "Hawaii", "Idaho", "Montana", "Nevada", "New Maxiko", "Oregon", "Utah", "Washington", "Wyoming"] 
    elif cur_dataset == 'east coast geo':
      state_list = ["Maine", "New Hampshire", "Massachusetts", "Rhode Island", "Connecticut", "New York", "New Jersey", "Delaware", "Maryland", "Virginia", "North Carolina", "South Carolina", "Georgia", "Florida"]
    elif cur_dataset == 'none coast geo':
      state_list = ["Alabama", "Arkansas", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", "Michigan", "Minnesota", "Mississippi", "Missouri", "Nebraska", "North Dakota", "Ohio", "Oklahoma", "Pennsylvania", "South Dakota", "Tennessee", "Texas", "Vermont", "West Virginia", "Wisconsin", "Puerto Rico", "Colorado", "Wyoming", "Montana", "Idaho", "Utah", "Nevada", "Arizona", "New Mexiko"]
    elif cur_dataset == 'none coast wiki':
      state_list = ["Alabama", "Arkansas", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", "Michigan", "Minnesota", "Mississippi", "Missouri", "Nebraska", "North Dakota", "Ohio", "Oklahoma", "Pennsylvania", "South Dakota", "Tennessee", "Texas", "Vermont", "West Virginia", "Wisconsin", "Puerto Rico", "Colorado", "Wyoming", "Montana", "Idaho", "Utah", "Nevada", "Arizona", "New Mexiko"]
    elif cur_dataset == 'urban':
      state_list = ["California ","New Jersey ","Nevada ","Puerto Rico ","Massachusetts ","Hawaii ","Florida","Rhode Island ","Utah ","Arizona ","Illinois ","Connecticut ","New York ","Maryland ","Colorado ","Texas ","Washington ","Delaware ","Oregon "]
    elif cur_dataset == 'rural':
      state_list = ["Pennsylvania", "Ohio", "New Mexico", "Virginia", "Georgia", "Michigan", "Kansas", "Minnesota", "Louisiana", "Nebraska", "Indiana", "Idaho", "Missouri", "Wisconsin", "Tennessee", "South Carolina", "Oklahoma", "North Carolina", "Alaska", "Wyoming", "Iowa", "New Hampshire", "North Dakota", "Alabama", "Kentucky", "South Dakota", "Arkansas", "Montana", "Mississippi", "West Virginia", "Vermont", "Maine", ]
    elif cur_dataset == 'north':
      state_list = ["Washington","Oregon","Idaho","Montana","Wyoming","North Dakota","South Dakota","Nebraska","Minnesota","Iowa","Wisconsin","Illinois","Michigan","Indiana","Ohio","Pennslyvania","New York","Vermont","New Hampshire","Maine","Massasuchets","Conniticet","New Jersey"]
    elif cur_dataset == 'south':
      state_list = ["California","Nevada","Utah","Arizona","New Mexico","Colorado","Texas","Oklahoma","Kansas","Arkansas","Missouri","Louisana","Missisipi","Alabama","Tenesee","Kentucky","Georgia","Florida","South Carolina","North Carolina","Virginia","West Virginia","Delaware","Maryland"]
    elif cur_dataset == 'all':
      state_list = ["California","Nevada","Utah","Arizona","New Mexico","Colorado","Texas","Oklahoma","Kansas","Arkansas","Missouri","Louisana","Missisipi","Alabama","Tenesee","Kentucky","Georgia","Florida","South Carolina","North Carolina","Virginia","West Virginia","Delaware","Maryland","Washington","Oregon","Idaho","Montana","Wyoming","North Dakota","South Dakota","Nebraska","Minnesota","Iowa","Wisconsin","Illinois","Michigan","Indiana","Ohio","Pennslyvania","New York","Vermont","New Hampshire","Maine","Massasuchets","Conniticet","New Jersey"]
    else: 
      state_list = []

    for cur_years in years:
      #Get the current combination of years
      splitted_years = cur_years.split()

      for threshold in thresholds:
        """
        if verbose == 0:
          features_orig,labels_orig,group_orig,track_list_year_orig,track_list_state_orig = deafen(get_income_data_with_track,state_list,splitted_years,threshold,roup_non_white)
        if verbose == 1:
          print('-------------------------------------------------')
          print('Creating dataset:',str(cur_dataset))
          print('-------------------------------------------------')
          features_orig,labels_orig,group_orig,track_list_year_orig,track_list_state_orig = deafen(get_income_data_with_track,state_list,splitted_years,threshold,group_non_white)
        if verbose == 2:
          features_orig,labels_orig,group_orig,track_list_year_orig,track_list_state_orig = get_income_data_with_track(state_list,splitted_years,threshold=threshold,group_non_white=group_non_white)
        """

        print('-------------------------------------------------')
        print('Creating dataset:',str(cur_dataset).replace(" ", "_") + str(splitted_years))
        print('-------------------------------------------------')
        features_orig,labels_orig,group_orig,track_list_year_orig,track_list_state_orig = get_income_data_with_track(state_list,splitted_years,threshold=threshold,group_non_white=group_non_white)

        df_all_orig = Get_Dataframe_from_all(features_orig,labels_orig)

        df_features = label_numpy_data(features_orig)

        aif_dataset = BinaryLabelDataset(df= df_all_orig,label_names = ["Label"],protected_attribute_names =['RAC1P'],privileged_protected_attributes = [1])

        split_value= 0.7  # splits the data set into train and test data
        dataset_orig_train, dataset_orig_test = aif_dataset.split([split_value], shuffle=True,seed=seed)

        list_of_all_datasets_train.append(dataset_orig_train)
        list_of_all_datasets_test.append(dataset_orig_test)
        name_of_datasets.append(cur_dataset)
        list_of_years.append(cur_years)
        list_of_thresholds.append(threshold)


        """
        Pickle Datasets
        """

        filename = 'Dataset_' + str(cur_dataset).replace(" ", "_") + '_' + str(cur_years) + '_' + str(threshold) + 'group_non_white_' + str(group_non_white) + '.pickle'
        outfile = open(filename,'wb')

        pickle.dump(dataset_orig_test,outfile)
        outfile.close()

        if verbose == 2:
          print('-------------------------------------------------')
          print('Finished pickling the dataset:',str(cur_dataset))
          print('-------------------------------------------------')

  """
  Print the datasets and lists obtained
  """

  #print('list_of_all_datasets_train',list_of_all_datasets_train)
  #print('list_of_all_datasets_test',list_of_all_datasets_test)
  #print('name_of_datasets',name_of_datasets)
  #print('list_of_years',list_of_years)
  #print('list_of_thresholds',list_of_thresholds)


  if verbose == 1 or verbose == 2:
    print('-------------------------------------------------')
    print('Finished creating the datasets')
    print('-------------------------------------------------')

  """
  Model section
  """
  #privileged_group = []

  #unprivileged_group = []

  if group_non_white == False:
    privileged_groups = [{'RAC1P': 1}]
    unprivileged_groups = [{'RAC1P': 2,'RAC1P': 3,'RAC1P': 4,'RAC1P': 5,'RAC1P': 6,'RAC1P': 7,'RAC1P': 8,'RAC1P': 9}]
  if group_non_white == True:
    privileged_groups = [{'RAC1P': 1}]
    unprivileged_groups = [{'RAC1P': -1}]

  list_of_metrics_from_last_model = []
  list_of_datasets_for_last_model = []

  if metric == 'disparate impact':
    metric_key = 'disparate_impact'
  else:
    metric_key = metric

  for cur_dataset_num in range(len(name_of_datasets)):
    for cur_model in models:
      if cur_model == 'adversial debiasing':
        """
        First learn the debiased model
        """
        sess = tf.compat.v1.Session()

        cur_name = 'adv_deb_'+ str(name_of_datasets[cur_dataset_num]).replace(" ", "_")+ str(list_of_years[cur_dataset_num]) + str(list_of_thresholds[cur_dataset_num]) +  '_debias' + str(random.randrange(1,10000))
        
        debias_model = AdversarialDebiasing(privileged_groups = privileged_groups,
                      unprivileged_groups = unprivileged_groups,
                      scope_name=cur_name,
                      debias=True,
                      sess=sess)

        #print(list_of_all_datasets_train[cur_dataset_num])

        if verbose == 0:
          deafen(debias_model.fit,list_of_all_datasets_train[cur_dataset_num])
        if verbose == 1:
          print('-------------------------------------------------')
          print('Now starting training of adv. Deb. --- debiased model on dataset:' + str(name_of_datasets[cur_dataset_num]) + str(list_of_years[cur_dataset_num]) + str(list_of_thresholds[cur_dataset_num]))
          print('-------------------------------------------------')
          deafen(debias_model.fit,list_of_all_datasets_train[cur_dataset_num])
        if verbose == 2:
          debias_model.fit(list_of_all_datasets_train[cur_dataset_num])

        """
        Second learn the biased model
        """

        cur_name = 'adv_deb_'+ str(name_of_datasets[cur_dataset_num]).replace(" ", "_") + str(list_of_years[cur_dataset_num]) + str(list_of_thresholds[cur_dataset_num]) + '_bias' + str(random.randrange(1,10000))
        
        bias_model = AdversarialDebiasing(privileged_groups = privileged_groups,
                      unprivileged_groups = unprivileged_groups,
                      scope_name=cur_name,
                      debias=False,
                      sess=sess)

        if verbose == 0:
          deafen(bias_model.fit,list_of_all_datasets_train[cur_dataset_num])
        if verbose == 1:
          print('-------------------------------------------------')
          print('Now starting training of adv. Deb. --- biased model on dataset:' + str(name_of_datasets[cur_dataset_num]) + ' year ' + str(list_of_years[cur_dataset_num]) + ' threshold ' + str(list_of_thresholds[cur_dataset_num]) )
          print('-------------------------------------------------')
          deafen(bias_model.fit,list_of_all_datasets_train[cur_dataset_num])
        if verbose == 2:
          bias_model.fit(list_of_all_datasets_train[cur_dataset_num])

        """
        Predict the model on all datasets
        """


        for cur_test_num in range(len(list_of_all_datasets_test)):
          preds_debias = debias_model.predict(list_of_all_datasets_test[cur_test_num])

          preds_bias = bias_model.predict(list_of_all_datasets_test[cur_test_num])


          """
          Pickle Predictions
          """
          filename = 'Preds_' + 'adv_deb_'+'debias_'+ str(name_of_datasets[cur_dataset_num]) + '_' + str(list_of_years[cur_dataset_num]) + '_' + str(list_of_thresholds[cur_dataset_num]) + '_' + str(name_of_datasets[cur_test_num]) + '_' + str(list_of_years[cur_test_num]) + '_' + str(list_of_thresholds[cur_test_num]) + '_' + 'group_non_white_' + str(group_non_white) + '.pickle'
          outfile = open(filename,'wb')

          pickle.dump(preds_debias,outfile)
          outfile.close()

          filename = 'Preds_' + 'adv_deb_'+'bias_'+ str(name_of_datasets[cur_dataset_num]) + '_' + str(list_of_years[cur_dataset_num]) + '_' + str(list_of_thresholds[cur_dataset_num]) + '_' + str(name_of_datasets[cur_test_num]) + '_' + str(list_of_years[cur_test_num]) + '_' + str(list_of_thresholds[cur_test_num]) + '_' + 'group_non_white_' + str(group_non_white) + '.pickle'
          outfile = open(filename,'wb')

          pickle.dump(preds_bias,outfile)
          outfile.close()


          if verbose == 1 or verbose == 2:
            print('-------------------------------------------------')
            print('Finished pickling the predictions')
            print('-------------------------------------------------')



        
          """
          Evaluate predictions
          """        
          #Debiased Model (returns dict and writes results to .csv file)
          csv_name = 'metrics_' + 'adv_deb_'+'debias_'+ str(name_of_datasets[cur_dataset_num]) + '_' + str(list_of_years[cur_dataset_num]) + '_' + str(list_of_thresholds[cur_dataset_num]) + '_' + str(name_of_datasets[cur_test_num]) + '_' + str(list_of_years[cur_test_num]) + '_' + str(list_of_thresholds[cur_test_num]) + '_' + 'group_non_white_' + str(group_non_white) + '.csv'
          model_name ='adv_deb_'+'debias_'+ str(name_of_datasets[cur_dataset_num]) + '_' + str(list_of_years[cur_dataset_num]) + '_' + str(list_of_thresholds[cur_dataset_num]) + '_' + str(name_of_datasets[cur_test_num]) + '_' + str(list_of_years[cur_test_num]) + '_' + str(list_of_thresholds[cur_test_num]) + '_' + 'group_non_white_' + str(group_non_white)


          metrics_dict = compute_metrics( list_of_all_datasets_test[cur_test_num],
                                          preds_debias,
                                          csv_name=csv_name,
                                          model_name = model_name,
                                          unprivileged_groups=unprivileged_groups,
                                          privileged_groups = privileged_groups
                                         )
          
          if verbose > 0:
            if list_of_years[cur_dataset_num] == list_of_years[-1]:
              if name_of_datasets[cur_dataset_num] == name_of_datasets[-1]:
                if list_of_thresholds[-1] == list_of_thresholds[cur_dataset_num]:
                  if list_of_years[cur_test_num] == list_of_years[-1]:
                    if list_of_thresholds[-1] == list_of_thresholds[cur_test_num]:
                    #Get the Results of the last trained model and get all metrics of datasets from the same year as the model (On the same threshold)
                      if metric_key == 'f1score':
                        cur_prec =  metrics_dict['precision']
                        cur_recall = metrics_dict['recall']
                        cur_f1score = 2 * (cur_prec * cur_recall) / (cur_prec + cur_recall) 
                        list_of_metrics_from_last_model.append(cur_f1score)
                      else: 
                        list_of_metrics_from_last_model.append(metrics_dict[metric_key])
                      list_of_datasets_for_last_model.append(name_of_datasets[cur_test_num])
            
          #Biased Model (returns dict and writes results to .csv file)

          csv_name = 'metrics_' + 'adv_deb_'+'bias_'+ str(name_of_datasets[cur_dataset_num]) + '_' + str(list_of_years[cur_dataset_num]) + '_' + str(list_of_thresholds[cur_dataset_num]) + '_' + str(name_of_datasets[cur_test_num]) + '_' + str(list_of_years[cur_test_num]) + '_' + str(list_of_thresholds[cur_test_num]) + '_' + 'group_non_white_' + str(group_non_white) + '.csv'
          model_name ='adv_deb_'+'bias_'+ str(name_of_datasets[cur_dataset_num]) + '_' + str(list_of_years[cur_dataset_num]) + '_' + str(list_of_thresholds[cur_dataset_num]) + '_' + str(name_of_datasets[cur_test_num]) + '_' + str(list_of_years[cur_test_num]) + '_' + str(list_of_thresholds[cur_test_num]) + '_' + 'group_non_white_' + str(group_non_white)

          metrics_dict = compute_metrics( list_of_all_datasets_test[cur_test_num],
                                          preds_bias,
                                          model_name = model_name,
                                          csv_name=csv_name,
                                          unprivileged_groups=unprivileged_groups,
                                          privileged_groups = privileged_groups
                                         )
          
          if verbose == 1 or verbose == 2:
            print('-------------------------------------------------')
            print('Finished computing the metrics')
            print('-------------------------------------------------')



        """
        Print progress
        """
        
        print('-------------------------------------------------')
        print('Finished Dataset: ',name_of_datasets[cur_dataset_num])
        print('-------------------------------------------------')


        """
        Remove remaining portions of the previous model
        """

        if cur_model == 'adversial debiasing':
          sess.close()

        print('-------------------------------------------------')
        print('Finished closing the session for adv. Deb.')
        print('-------------------------------------------------')


    if cur_model == 'Metafair':
      meta_fair_sr = MetaFairClassifier(sensitive_attr="RAC1P",type="sr",seed = seed)

      """
      Train the model
      """
      if verbose == 0:
          deafen(meta_fair_sr.fit,list_of_all_datasets_train[cur_dataset_num])
      if verbose == 1:
        print('-------------------------------------------------')
        print('Now starting training of Metafair model on dataset:' + str(name_of_datasets[cur_dataset_num]) + ' year ' + str(list_of_years[cur_dataset_num]) + ' threshold ' + str(list_of_thresholds[cur_dataset_num]) )
        print('-------------------------------------------------')
        deafen(meta_fair_sr.fit,list_of_all_datasets_train[cur_dataset_num])
      if verbose == 2:
        meta_fair_sr.fit(list_of_all_datasets_train[cur_dataset_num])


      """
      Predict the model on all datasets
      """


      for cur_test_num in range(len(list_of_all_datasets_test)):
        preds_metafair = meta_fair_sr.predict(list_of_all_datasets_test[cur_test_num])

      

        """
        Pickle Predictions
        """
        filename = 'Preds_' + 'Metafair_'+ str(name_of_datasets[cur_dataset_num]) + '_' + str(list_of_years[cur_dataset_num]) + '_' + str(list_of_thresholds[cur_dataset_num]) + '_' + str(name_of_datasets[cur_test_num]) + '_' + str(list_of_years[cur_test_num]) + '_' + str(list_of_thresholds[cur_test_num]) + '_' + 'group_non_white_' + str(group_non_white) + '.pickle'
        outfile = open(filename,'wb')

        pickle.dump(preds_metafair,outfile)
        outfile.close()


        if verbose == 1 or verbose == 2:
          print('-------------------------------------------------')
          print('Finished pickling the predictions')
          print('-------------------------------------------------')



        
        """
        Evaluate predictions
        """        
        #Meta fair 
        csv_name = 'metrics_' + 'metafair_'+ str(name_of_datasets[cur_dataset_num]) + '_' + str(list_of_years[cur_dataset_num]) + '_' + str(list_of_thresholds[cur_dataset_num]) + '_' + str(name_of_datasets[cur_test_num]) + '_' + str(list_of_years[cur_test_num]) + '_' + str(list_of_thresholds[cur_test_num]) + '_' + 'group_non_white_' + str(group_non_white) + '.csv'
        model_name = 'metafair_' + str(name_of_datasets[cur_dataset_num]) + '_' + str(list_of_years[cur_dataset_num]) + '_' + str(list_of_thresholds[cur_dataset_num]) + '_' + str(name_of_datasets[cur_test_num]) + '_' + str(list_of_years[cur_test_num]) + '_' + str(list_of_thresholds[cur_test_num]) + '_' + 'group_non_white_' + str(group_non_white)


        metrics_dict = compute_metrics( list_of_all_datasets_test[cur_test_num],
                                        preds_metafair,
                                        csv_name=csv_name,
                                        model_name = model_name,
                                        unprivileged_groups=unprivileged_groups,
                                        privileged_groups = privileged_groups
                                      )
        
        if verbose > 0:
          if list_of_years[cur_dataset_num] == list_of_years[-1]:
            if name_of_datasets[cur_dataset_num] == name_of_datasets[-1]:
              if list_of_thresholds[-1] == list_of_thresholds[cur_dataset_num]:
                if list_of_years[cur_test_num] == list_of_years[-1]:
                  if list_of_thresholds[-1] == list_of_thresholds[cur_test_num]:
                  #Get the Results of the last trained model and get all metrics of datasets from the same year as the model (On the same threshold)
                    if metric_key == 'f1score':
                      cur_prec =  metrics_dict['precision']
                      cur_recall = metrics_dict['recall']
                      cur_f1score = 2 * (cur_prec * cur_recall) / (cur_prec + cur_recall) 
                      list_of_metrics_from_last_model.append(cur_f1score)
                    else: 
                      list_of_metrics_from_last_model.append(metrics_dict[metric_key])
                    list_of_datasets_for_last_model.append(name_of_datasets[cur_test_num])


    if cur_model == 'PrejudiceRemover':
      prejudiceremover_model= PrejudiceRemover(sensitive_attr="RAC1P")

      """
      Train the model
      """
      if verbose == 0:
          deafen(prejudiceremover_model.fit,list_of_all_datasets_train[cur_dataset_num])
      if verbose == 1:
        print('-------------------------------------------------')
        print('Now starting training of PrejudiceRemover model on dataset:' + str(name_of_datasets[cur_dataset_num]) + ' year ' + str(list_of_years[cur_dataset_num]) + ' threshold ' + str(list_of_thresholds[cur_dataset_num]) )
        print('-------------------------------------------------')
        deafen(prejudiceremover_model.fit,list_of_all_datasets_train[cur_dataset_num])
      if verbose == 2:
        prejudiceremover_model.fit(list_of_all_datasets_train[cur_dataset_num])


      """
      Predict the model on all datasets
      """


      for cur_test_num in range(len(list_of_all_datasets_test)):
        preds_prejudiceremover = prejudiceremover_model.predict(list_of_all_datasets_test[cur_test_num])

      

        """
        Pickle Predictions
        """
        filename = 'Preds_' + 'prejudiceremover_'+ str(name_of_datasets[cur_dataset_num]) + '_' + str(list_of_years[cur_dataset_num]) + '_' + str(list_of_thresholds[cur_dataset_num]) + '_' + str(name_of_datasets[cur_test_num]) + '_' + str(list_of_years[cur_test_num]) + '_' + str(list_of_thresholds[cur_test_num]) + '_' + 'group_non_white_' + str(group_non_white) + '.pickle'
        outfile = open(filename,'wb')

        pickle.dump(preds_prejudiceremover,outfile)
        outfile.close()


        if verbose == 1 or verbose == 2:
          print('-------------------------------------------------')
          print('Finished pickling the predictions')
          print('-------------------------------------------------')



        
        """
        Evaluate predictions
        """        
        #Meta fair 
        csv_name = 'metrics_' + 'PrejudiceRemover_'+ str(name_of_datasets[cur_dataset_num]) + '_' + str(list_of_years[cur_dataset_num]) + '_' + str(list_of_thresholds[cur_dataset_num]) + '_' + str(name_of_datasets[cur_test_num]) + '_' + str(list_of_years[cur_test_num]) + '_' + str(list_of_thresholds[cur_test_num]) + '_' + 'group_non_white_' + str(group_non_white) + '.csv'
        model_name = 'PrejudiceRemover_'+ str(name_of_datasets[cur_dataset_num]) + '_' + str(list_of_years[cur_dataset_num]) + '_' + str(list_of_thresholds[cur_dataset_num]) + '_' + str(name_of_datasets[cur_test_num]) + '_' + str(list_of_years[cur_test_num]) + '_' + str(list_of_thresholds[cur_test_num]) + '_' + 'group_non_white_' + str(group_non_white)


        metrics_dict = compute_metrics( list_of_all_datasets_test[cur_test_num],
                                        preds_prejudiceremover,
                                        csv_name=csv_name,
                                        model_name = model_name,
                                        unprivileged_groups=unprivileged_groups,
                                        privileged_groups = privileged_groups
                                      )
        if verbose > 0:
          if list_of_years[cur_dataset_num] == list_of_years[-1]:
            if name_of_datasets[cur_dataset_num] == name_of_datasets[-1]:
              if list_of_thresholds[-1] == list_of_thresholds[cur_dataset_num]:
                if list_of_years[cur_test_num] == list_of_years[-1]:
                  if list_of_thresholds[-1] == list_of_thresholds[cur_test_num]:
                  #Get the Results of the last trained model and get all metrics of datasets from the same year as the model (On the same threshold)
                    if metric_key == 'f1score':
                      cur_prec =  metrics_dict['precision']
                      cur_recall = metrics_dict['recall']
                      cur_f1score = 2 * (cur_prec * cur_recall) / (cur_prec + cur_recall) 
                      list_of_metrics_from_last_model.append(cur_f1score)
                    else: 
                      list_of_metrics_from_last_model.append(metrics_dict[metric_key])
                    list_of_datasets_for_last_model.append(name_of_datasets[cur_test_num])



  if verbose == 2:
    print('-------------------------------------------------')
    print('Now printing the last metrics Dictionary')
    print(metrics_dict)
    print('-------------------------------------------------')
    
    

  if verbose == 1 or verbose == 2:
    print('-------------------------------------------------')
    print('Thank you for using this model. The Grid Training is now complete.')
    print('-------------------------------------------------')
    print('Now outputting the generated plots for the metrics of the latest trained model')
    if models[-1:] == 'adversial debiasing':
       print('Model:',models[-1:],'(Note: We only use the metrics of the debiased model)')
    else:
       print('Model:',models[-1:])
    print('Trained on Year:',str(list_of_years[-1:]),'Dataset',str(name_of_datasets[-1:]),'Threshold:',str(list_of_thresholds[-1:]))
    #print('Trained on Year:',list_of_years,'Datasets',,'Threshold:',))

    print('-------------------------------------------------')
    #print(list_of_datasets_for_last_model)
    #print(list_of_metrics_from_last_model)
    #print(metric)
    #print(file_name)

    information_dict = {
            "trained_dataset" : str(name_of_datasets[-1]),
            "year" : str(list_of_years[-1]),
            "threshold" : str(list_of_thresholds[-1]),
            "model_name" : models[-1],
    }

    plot_us_regions_and_states(list_of_datasets_for_last_model,list_of_metrics_from_last_model,infos = information_dict ,metric = metric,file_name_pre=file_prefix)
    
    print('-------------------------------------------------')

"""---
# üß© Grid Training Examples

This section shows the wide range of possible inputs our Grid Training framework can utilize.

---


When the setup finished, you can <font color='orange'>run the cell below</font> to start the interactive grid training.

Here, you can easily generate new data by setting your own parameters. Simply select all of the parameters you are interested in and then the <font color='orange'>run the cell below the paramters</font>. The Training will start.

Note: The interesting information is in the metrics csv file.

**Interpretation Guidelines**

<table>
  <tr>
    <th>Parameter</th>
    <th>Meaning</th>
  </tr>
  <tr>
    <td>Evaluation Mode</td>
    <td>Evaluate different models tested on the same dataset => mode <i>'test'</i> <br></br>Evaluate model trained on one dataset on multiple other datasets => mode <i>'train'</i></td>
  </tr>
  <tr>
    <td>Evaluation Dataset</td>
    <td>For which dataset do you want to explore metrics?</td>
  </tr>
  <tr>
    <td>Evaluation Year</td>
    <td>For which years do you want to explore metrics?</td>
  </tr>
  <tr>
    <td>Income Threshold</td>
    <td>For which threshold do you want to explore metrics?</td>
  </tr>
  <tr>
    <td>Metric</td>
    <td>Which metric do you want to plot?<br></br>Click <i>'Show Advanced Metrics'</i> to see all collected metrics.</td>
  </tr>
  <tr>
    <td>Legend Position</td>
    <td>(Where) do you want the legend to be placed?<br></br>Choose <i>'best'</i> for automatic placement.</td>
  </tr>  
</table>
<br/><br/>

**Have fun exploring the Data!**
"""

datasets = ['rural']

years = ['2014']

models = ['Metafair'] 

thresholds = ['50000']

group_non_white = True 

verbose = 2 

metric='accuracy'

grid_train_model_v2(datasets,years, models,thresholds,group_non_white,verbose)